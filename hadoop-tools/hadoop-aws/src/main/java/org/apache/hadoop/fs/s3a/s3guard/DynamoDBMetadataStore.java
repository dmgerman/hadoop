begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
DECL|package|org.apache.hadoop.fs.s3a.s3guard
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|s3guard
package|;
end_package

begin_import
import|import
name|javax
operator|.
name|annotation
operator|.
name|Nullable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InterruptedIOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|AccessDeniedException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Objects
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|CompletableFuture
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|TimeUnit
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicInteger
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicLong
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicReference
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|stream
operator|.
name|Collectors
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|AmazonServiceException
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|auth
operator|.
name|AWSCredentialsProvider
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|AmazonDynamoDB
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|BatchWriteItemOutcome
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|DynamoDB
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|Item
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|ItemCollection
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|PrimaryKey
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|PutItemOutcome
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|QueryOutcome
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|ScanOutcome
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|TableWriteItems
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|spec
operator|.
name|GetItemSpec
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|spec
operator|.
name|QuerySpec
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|utils
operator|.
name|ValueMap
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|model
operator|.
name|AmazonDynamoDBException
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|model
operator|.
name|ProvisionedThroughputDescription
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|model
operator|.
name|TableDescription
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|model
operator|.
name|WriteRequest
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ListeningExecutorService
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang3
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|classification
operator|.
name|InterfaceAudience
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|classification
operator|.
name|InterfaceStability
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathIOException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|RemoteIterator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|AWSCredentialProviderList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|AWSServiceThrottledException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|Constants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|Invoker
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|Retries
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|S3AFileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|S3AFileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|S3AInstrumentation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|S3AUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|Tristate
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|auth
operator|.
name|RoleModel
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|auth
operator|.
name|RolePolicies
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|auth
operator|.
name|delegation
operator|.
name|AWSPolicyProvider
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|impl
operator|.
name|StoreContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|retry
operator|.
name|RetryPolicies
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|retry
operator|.
name|RetryPolicy
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|BlockingThreadPoolExecutorService
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|DurationInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|ReflectionUtils
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|Constants
operator|.
name|*
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|S3AUtils
operator|.
name|*
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|auth
operator|.
name|RolePolicies
operator|.
name|allowAllDynamoDBOperations
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|auth
operator|.
name|RolePolicies
operator|.
name|allowS3GuardClientOperations
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|impl
operator|.
name|CallableSupplier
operator|.
name|submit
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|impl
operator|.
name|CallableSupplier
operator|.
name|waitForCompletion
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|s3guard
operator|.
name|PathMetadataDynamoDBTranslation
operator|.
name|*
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|s3guard
operator|.
name|S3Guard
operator|.
name|*
import|;
end_import

begin_comment
comment|/**  * DynamoDBMetadataStore is a {@link MetadataStore} that persists  * file system metadata to DynamoDB.  *  * The current implementation uses a schema consisting of a single table.  The  * name of the table can be configured by config key  * {@link org.apache.hadoop.fs.s3a.Constants#S3GUARD_DDB_TABLE_NAME_KEY}.  * By default, it matches the name of the S3 bucket.  Each item in the table  * represents a single directory or file.  Its path is split into separate table  * attributes:  *<ul>  *<li> parent (absolute path of the parent, with bucket name inserted as  * first path component).</li>  *<li> child (path of that specific child, relative to parent).</li>  *<li> optional boolean attribute tracking whether the path is a directory.  *      Absence or a false value indicates the path is a file.</li>  *<li> optional long attribute revealing modification time of file.  *      This attribute is meaningful only to file items.</li>  *<li> optional long attribute revealing file length.  *      This attribute is meaningful only to file items.</li>  *<li> optional long attribute revealing block size of the file.  *      This attribute is meaningful only to file items.</li>  *<li> optional string attribute tracking the s3 eTag of the file.  *      May be absent if the metadata was entered with a version of S3Guard  *      before this was tracked.  *      This attribute is meaningful only to file items.</li>   *<li> optional string attribute tracking the s3 versionId of the file.  *      May be absent if the metadata was entered with a version of S3Guard  *      before this was tracked.  *      This attribute is meaningful only to file items.</li>  *</ul>  *  * The DynamoDB partition key is the parent, and the range key is the child.  *  * To allow multiple buckets to share the same DynamoDB table, the bucket  * name is treated as the root directory.  *  * For example, assume the consistent store contains metadata representing this  * file system structure:  *  *<pre>  * s3a://bucket/dir1  * |-- dir2  * |   |-- file1  * |   `-- file2  * `-- dir3  *     |-- dir4  *     |   `-- file3  *     |-- dir5  *     |   `-- file4  *     `-- dir6  *</pre>  *  * This is persisted to a single DynamoDB table as:  *  *<pre>  * ====================================================================================  * | parent                 | child | is_dir | mod_time | len | etag | ver_id |  ...  |  * ====================================================================================  * | /bucket                | dir1  | true   |          |     |      |        |       |  * | /bucket/dir1           | dir2  | true   |          |     |      |        |       |  * | /bucket/dir1           | dir3  | true   |          |     |      |        |       |  * | /bucket/dir1/dir2      | file1 |        |   100    | 111 | abc  |  mno   |       |  * | /bucket/dir1/dir2      | file2 |        |   200    | 222 | def  |  pqr   |       |  * | /bucket/dir1/dir3      | dir4  | true   |          |     |      |        |       |  * | /bucket/dir1/dir3      | dir5  | true   |          |     |      |        |       |  * | /bucket/dir1/dir3/dir4 | file3 |        |   300    | 333 | ghi  |  stu   |       |  * | /bucket/dir1/dir3/dir5 | file4 |        |   400    | 444 | jkl  |  vwx   |       |  * | /bucket/dir1/dir3      | dir6  | true   |          |     |      |        |       |  * ====================================================================================  *</pre>  *  * This choice of schema is efficient for read access patterns.  * {@link #get(Path)} can be served from a single item lookup.  * {@link #listChildren(Path)} can be served from a query against all rows  * matching the parent (the partition key) and the returned list is guaranteed  * to be sorted by child (the range key).  Tracking whether or not a path is a  * directory helps prevent unnecessary queries during traversal of an entire  * sub-tree.  *  * Some mutating operations, notably  * {@link MetadataStore#deleteSubtree(Path, BulkOperationState)} and  * {@link MetadataStore#move(Collection, Collection, BulkOperationState)}  * are less efficient with this schema.  * They require mutating multiple items in the DynamoDB table.  *  * By default, DynamoDB access is performed within the same AWS region as  * the S3 bucket that hosts the S3A instance.  During initialization, it checks  * the location of the S3 bucket and creates a DynamoDB client connected to the  * same region. The region may also be set explicitly by setting the config  * parameter {@code fs.s3a.s3guard.ddb.region} to the corresponding region.  */
end_comment

begin_class
annotation|@
name|InterfaceAudience
operator|.
name|Private
annotation|@
name|InterfaceStability
operator|.
name|Evolving
DECL|class|DynamoDBMetadataStore
specifier|public
class|class
name|DynamoDBMetadataStore
implements|implements
name|MetadataStore
implements|,
name|AWSPolicyProvider
block|{
DECL|field|LOG
specifier|public
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|DynamoDBMetadataStore
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/**    * Name of the operations log.    */
DECL|field|OPERATIONS_LOG_NAME
specifier|public
specifier|static
specifier|final
name|String
name|OPERATIONS_LOG_NAME
init|=
literal|"org.apache.hadoop.fs.s3a.s3guard.Operations"
decl_stmt|;
comment|/**    * A log of all state changing operations to the store;    * only updated at debug level.    */
DECL|field|OPERATIONS_LOG
specifier|public
specifier|static
specifier|final
name|Logger
name|OPERATIONS_LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|OPERATIONS_LOG_NAME
argument_list|)
decl_stmt|;
comment|/** parent/child name to use in the version marker. */
DECL|field|VERSION_MARKER_ITEM_NAME
specifier|public
specifier|static
specifier|final
name|String
name|VERSION_MARKER_ITEM_NAME
init|=
literal|"../VERSION"
decl_stmt|;
comment|/** parent/child name to use in the version marker. */
DECL|field|VERSION_MARKER_TAG_NAME
specifier|public
specifier|static
specifier|final
name|String
name|VERSION_MARKER_TAG_NAME
init|=
literal|"s3guard_version"
decl_stmt|;
comment|/** Current version number. */
DECL|field|VERSION
specifier|public
specifier|static
specifier|final
name|int
name|VERSION
init|=
literal|100
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|BILLING_MODE
specifier|static
specifier|final
name|String
name|BILLING_MODE
init|=
literal|"billing-mode"
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|BILLING_MODE_PER_REQUEST
specifier|static
specifier|final
name|String
name|BILLING_MODE_PER_REQUEST
init|=
literal|"per-request"
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|BILLING_MODE_PROVISIONED
specifier|static
specifier|final
name|String
name|BILLING_MODE_PROVISIONED
init|=
literal|"provisioned"
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|DESCRIPTION
specifier|static
specifier|final
name|String
name|DESCRIPTION
init|=
literal|"S3Guard metadata store in DynamoDB"
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|READ_CAPACITY
specifier|static
specifier|final
name|String
name|READ_CAPACITY
init|=
literal|"read-capacity"
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|WRITE_CAPACITY
specifier|static
specifier|final
name|String
name|WRITE_CAPACITY
init|=
literal|"write-capacity"
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|STATUS
specifier|static
specifier|final
name|String
name|STATUS
init|=
literal|"status"
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|TABLE
specifier|static
specifier|final
name|String
name|TABLE
init|=
literal|"table"
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|HINT_DDB_IOPS_TOO_LOW
specifier|static
specifier|final
name|String
name|HINT_DDB_IOPS_TOO_LOW
init|=
literal|" This may be because the write threshold of DynamoDB is set too low."
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|THROTTLING
specifier|static
specifier|final
name|String
name|THROTTLING
init|=
literal|"Throttling"
decl_stmt|;
DECL|field|E_ON_DEMAND_NO_SET_CAPACITY
specifier|public
specifier|static
specifier|final
name|String
name|E_ON_DEMAND_NO_SET_CAPACITY
init|=
literal|"Neither ReadCapacityUnits nor WriteCapacityUnits can be specified when BillingMode is PAY_PER_REQUEST"
decl_stmt|;
annotation|@
name|VisibleForTesting
DECL|field|E_INCONSISTENT_UPDATE
specifier|static
specifier|final
name|String
name|E_INCONSISTENT_UPDATE
init|=
literal|"Duplicate and inconsistent entry in update operation"
decl_stmt|;
DECL|field|DELETE_TRACKING_VALUE_MAP
specifier|private
specifier|static
specifier|final
name|ValueMap
name|DELETE_TRACKING_VALUE_MAP
init|=
operator|new
name|ValueMap
argument_list|()
operator|.
name|withBoolean
argument_list|(
literal|":false"
argument_list|,
literal|false
argument_list|)
decl_stmt|;
comment|/**    * The maximum number of outstanding operations to submit    * before blocking to await completion of all the executors.    * Paging work like this is less efficient, but it ensures that    * failure (auth, network, etc) are picked up before many more    * operations are submitted.    *    * Arbitrary Choice.    * Value: {@value}.    */
DECL|field|S3GUARD_DDB_SUBMITTED_TASK_LIMIT
specifier|private
specifier|static
specifier|final
name|int
name|S3GUARD_DDB_SUBMITTED_TASK_LIMIT
init|=
literal|50
decl_stmt|;
DECL|field|amazonDynamoDB
specifier|private
name|AmazonDynamoDB
name|amazonDynamoDB
decl_stmt|;
DECL|field|dynamoDB
specifier|private
name|DynamoDB
name|dynamoDB
decl_stmt|;
DECL|field|credentials
specifier|private
name|AWSCredentialProviderList
name|credentials
decl_stmt|;
DECL|field|region
specifier|private
name|String
name|region
decl_stmt|;
DECL|field|table
specifier|private
name|Table
name|table
decl_stmt|;
DECL|field|tableName
specifier|private
name|String
name|tableName
decl_stmt|;
DECL|field|conf
specifier|private
name|Configuration
name|conf
decl_stmt|;
DECL|field|username
specifier|private
name|String
name|username
decl_stmt|;
comment|/**    * This policy is mostly for batched writes, not for processing    * exceptions in invoke() calls.    * It also has a role purpose in    * {@link DynamoDBMetadataStoreTableManager#getVersionMarkerItem()};    * look at that method for the details.    */
DECL|field|batchWriteRetryPolicy
specifier|private
name|RetryPolicy
name|batchWriteRetryPolicy
decl_stmt|;
DECL|field|instrumentation
specifier|private
name|S3AInstrumentation
operator|.
name|S3GuardInstrumentation
name|instrumentation
decl_stmt|;
comment|/** Owner FS: only valid if configured with an owner FS. */
DECL|field|owner
specifier|private
name|S3AFileSystem
name|owner
decl_stmt|;
comment|/** Invoker for IO. Until configured properly, use try-once. */
DECL|field|invoker
specifier|private
name|Invoker
name|invoker
init|=
operator|new
name|Invoker
argument_list|(
name|RetryPolicies
operator|.
name|TRY_ONCE_THEN_FAIL
argument_list|,
name|Invoker
operator|.
name|NO_OP
argument_list|)
decl_stmt|;
comment|/** Invoker for read operations. */
DECL|field|readOp
specifier|private
name|Invoker
name|readOp
decl_stmt|;
comment|/** Invoker for write operations. */
DECL|field|writeOp
specifier|private
name|Invoker
name|writeOp
decl_stmt|;
DECL|field|readThrottleEvents
specifier|private
specifier|final
name|AtomicLong
name|readThrottleEvents
init|=
operator|new
name|AtomicLong
argument_list|(
literal|0
argument_list|)
decl_stmt|;
DECL|field|writeThrottleEvents
specifier|private
specifier|final
name|AtomicLong
name|writeThrottleEvents
init|=
operator|new
name|AtomicLong
argument_list|(
literal|0
argument_list|)
decl_stmt|;
DECL|field|batchWriteCapacityExceededEvents
specifier|private
specifier|final
name|AtomicLong
name|batchWriteCapacityExceededEvents
init|=
operator|new
name|AtomicLong
argument_list|(
literal|0
argument_list|)
decl_stmt|;
comment|/**    * Total limit on the number of throttle events after which    * we stop warning in the log. Keeps the noise down.    */
DECL|field|THROTTLE_EVENT_LOG_LIMIT
specifier|private
specifier|static
specifier|final
name|int
name|THROTTLE_EVENT_LOG_LIMIT
init|=
literal|100
decl_stmt|;
comment|/**    * Count of the total number of throttle events; used to crank back logging.    */
DECL|field|throttleEventCount
specifier|private
name|AtomicInteger
name|throttleEventCount
init|=
operator|new
name|AtomicInteger
argument_list|(
literal|0
argument_list|)
decl_stmt|;
comment|/**    * Executor for submitting operations.    */
DECL|field|executor
specifier|private
name|ListeningExecutorService
name|executor
decl_stmt|;
comment|/**    * Time source. This is used during writes when parent    * entries need to be created.    */
DECL|field|ttlTimeProvider
specifier|private
name|ITtlTimeProvider
name|ttlTimeProvider
decl_stmt|;
DECL|field|tableHandler
specifier|private
name|DynamoDBMetadataStoreTableManager
name|tableHandler
decl_stmt|;
comment|/**    * A utility function to create DynamoDB instance.    * @param conf the file system configuration    * @param s3Region region of the associated S3 bucket (if any).    * @param bucket Optional bucket to use to look up per-bucket proxy secrets    * @param credentials credentials.    * @return DynamoDB instance.    * @throws IOException I/O error.    */
DECL|method|createDynamoDB ( final Configuration conf, final String s3Region, final String bucket, final AWSCredentialsProvider credentials)
specifier|private
name|DynamoDB
name|createDynamoDB
parameter_list|(
specifier|final
name|Configuration
name|conf
parameter_list|,
specifier|final
name|String
name|s3Region
parameter_list|,
specifier|final
name|String
name|bucket
parameter_list|,
specifier|final
name|AWSCredentialsProvider
name|credentials
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|amazonDynamoDB
operator|==
literal|null
condition|)
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|conf
argument_list|)
expr_stmt|;
specifier|final
name|Class
argument_list|<
name|?
extends|extends
name|DynamoDBClientFactory
argument_list|>
name|cls
init|=
name|conf
operator|.
name|getClass
argument_list|(
name|S3GUARD_DDB_CLIENT_FACTORY_IMPL
argument_list|,
name|S3GUARD_DDB_CLIENT_FACTORY_IMPL_DEFAULT
argument_list|,
name|DynamoDBClientFactory
operator|.
name|class
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Creating DynamoDB client {} with S3 region {}"
argument_list|,
name|cls
argument_list|,
name|s3Region
argument_list|)
expr_stmt|;
name|amazonDynamoDB
operator|=
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|cls
argument_list|,
name|conf
argument_list|)
operator|.
name|createDynamoDBClient
argument_list|(
name|s3Region
argument_list|,
name|bucket
argument_list|,
name|credentials
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|DynamoDB
argument_list|(
name|amazonDynamoDB
argument_list|)
return|;
block|}
comment|/**    * {@inheritDoc}.    * The credentials for authenticating with S3 are requested from the    * FS via {@link S3AFileSystem#shareCredentials(String)}; this will    * increment the reference counter of these credentials.    * @param fs {@code S3AFileSystem} associated with the MetadataStore    * @param ttlTp the time provider to use for metadata expiry    * @throws IOException on a failure    */
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|OnceRaw
DECL|method|initialize (FileSystem fs, ITtlTimeProvider ttlTp)
specifier|public
name|void
name|initialize
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|ITtlTimeProvider
name|ttlTp
parameter_list|)
throws|throws
name|IOException
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|fs
argument_list|,
literal|"Null filesystem"
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|fs
operator|instanceof
name|S3AFileSystem
argument_list|,
literal|"DynamoDBMetadataStore only supports S3A filesystem."
argument_list|)
expr_stmt|;
name|bindToOwnerFilesystem
argument_list|(
operator|(
name|S3AFileSystem
operator|)
name|fs
argument_list|)
expr_stmt|;
specifier|final
name|String
name|bucket
init|=
name|owner
operator|.
name|getBucket
argument_list|()
decl_stmt|;
name|String
name|confRegion
init|=
name|conf
operator|.
name|getTrimmed
argument_list|(
name|S3GUARD_DDB_REGION_KEY
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|confRegion
argument_list|)
condition|)
block|{
name|region
operator|=
name|confRegion
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Overriding S3 region with configured DynamoDB region: {}"
argument_list|,
name|region
argument_list|)
expr_stmt|;
block|}
else|else
block|{
try|try
block|{
name|region
operator|=
name|owner
operator|.
name|getBucketLocation
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AccessDeniedException
name|e
parameter_list|)
block|{
comment|// access denied here == can't call getBucket. Report meaningfully
name|URI
name|uri
init|=
name|owner
operator|.
name|getUri
argument_list|()
decl_stmt|;
name|String
name|message
init|=
literal|"Failed to get bucket location as client lacks permission "
operator|+
name|RolePolicies
operator|.
name|S3_GET_BUCKET_LOCATION
operator|+
literal|" for "
operator|+
name|uri
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|message
argument_list|)
expr_stmt|;
throw|throw
operator|(
name|IOException
operator|)
operator|new
name|AccessDeniedException
argument_list|(
name|message
argument_list|)
operator|.
name|initCause
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Inferring DynamoDB region from S3 bucket: {}"
argument_list|,
name|region
argument_list|)
expr_stmt|;
block|}
name|credentials
operator|=
name|owner
operator|.
name|shareCredentials
argument_list|(
literal|"s3guard"
argument_list|)
expr_stmt|;
name|dynamoDB
operator|=
name|createDynamoDB
argument_list|(
name|conf
argument_list|,
name|region
argument_list|,
name|bucket
argument_list|,
name|credentials
argument_list|)
expr_stmt|;
comment|// use the bucket as the DynamoDB table name if not specified in config
name|tableName
operator|=
name|conf
operator|.
name|getTrimmed
argument_list|(
name|S3GUARD_DDB_TABLE_NAME_KEY
argument_list|,
name|bucket
argument_list|)
expr_stmt|;
name|initDataAccessRetries
argument_list|(
name|conf
argument_list|)
expr_stmt|;
comment|// set up a full retry policy
name|invoker
operator|=
operator|new
name|Invoker
argument_list|(
operator|new
name|S3GuardDataAccessRetryPolicy
argument_list|(
name|conf
argument_list|)
argument_list|,
name|this
operator|::
name|retryEvent
argument_list|)
expr_stmt|;
name|this
operator|.
name|ttlTimeProvider
operator|=
name|ttlTp
expr_stmt|;
name|tableHandler
operator|=
operator|new
name|DynamoDBMetadataStoreTableManager
argument_list|(
name|dynamoDB
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|amazonDynamoDB
argument_list|,
name|conf
argument_list|,
name|readOp
argument_list|,
name|batchWriteRetryPolicy
argument_list|)
expr_stmt|;
name|this
operator|.
name|table
operator|=
name|tableHandler
operator|.
name|initTable
argument_list|()
expr_stmt|;
name|instrumentation
operator|.
name|initialized
argument_list|()
expr_stmt|;
block|}
comment|/**    * Declare that this table is owned by the specific S3A FS instance.    * This will bind some fields to the values provided by the owner,    * including wiring up the instrumentation.    * @param fs owner filesystem    */
annotation|@
name|VisibleForTesting
DECL|method|bindToOwnerFilesystem (final S3AFileSystem fs)
name|void
name|bindToOwnerFilesystem
parameter_list|(
specifier|final
name|S3AFileSystem
name|fs
parameter_list|)
block|{
name|owner
operator|=
name|fs
expr_stmt|;
name|conf
operator|=
name|owner
operator|.
name|getConf
argument_list|()
expr_stmt|;
name|StoreContext
name|context
init|=
name|owner
operator|.
name|createStoreContext
argument_list|()
decl_stmt|;
name|instrumentation
operator|=
name|context
operator|.
name|getInstrumentation
argument_list|()
operator|.
name|getS3GuardInstrumentation
argument_list|()
expr_stmt|;
name|username
operator|=
name|context
operator|.
name|getUsername
argument_list|()
expr_stmt|;
name|executor
operator|=
name|context
operator|.
name|createThrottledExecutor
argument_list|()
expr_stmt|;
name|ttlTimeProvider
operator|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|context
operator|.
name|getTimeProvider
argument_list|()
argument_list|,
literal|"ttlTimeProvider must not be null"
argument_list|)
expr_stmt|;
block|}
comment|/**    * Performs one-time initialization of the metadata store via configuration.    *    * This initialization depends on the configuration object to get AWS    * credentials, DynamoDBFactory implementation class, DynamoDB endpoints,    * DynamoDB table names etc. After initialization, this metadata store does    * not explicitly relate to any S3 bucket, which be nonexistent.    *    * This is used to operate the metadata store directly beyond the scope of the    * S3AFileSystem integration, e.g. command line tools.    * Generally, callers should use    * {@link MetadataStore#initialize(FileSystem, ITtlTimeProvider)}    * with an initialized {@code S3AFileSystem} instance.    *    * Without a filesystem to act as a reference point, the configuration itself    * must declare the table name and region in the    * {@link Constants#S3GUARD_DDB_TABLE_NAME_KEY} and    * {@link Constants#S3GUARD_DDB_REGION_KEY} respectively.    * It also creates a new credential provider list from the configuration,    * using the base fs.s3a.* options, as there is no bucket to infer per-bucket    * settings from.    *    * @see MetadataStore#initialize(FileSystem, ITtlTimeProvider)    * @throws IOException if there is an error    * @throws IllegalArgumentException if the configuration is incomplete    */
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|OnceRaw
DECL|method|initialize (Configuration config, ITtlTimeProvider ttlTp)
specifier|public
name|void
name|initialize
parameter_list|(
name|Configuration
name|config
parameter_list|,
name|ITtlTimeProvider
name|ttlTp
parameter_list|)
throws|throws
name|IOException
block|{
name|conf
operator|=
name|config
expr_stmt|;
comment|// use the bucket as the DynamoDB table name if not specified in config
name|tableName
operator|=
name|conf
operator|.
name|getTrimmed
argument_list|(
name|S3GUARD_DDB_TABLE_NAME_KEY
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|tableName
argument_list|)
argument_list|,
literal|"No DynamoDB table name configured"
argument_list|)
expr_stmt|;
name|region
operator|=
name|conf
operator|.
name|getTrimmed
argument_list|(
name|S3GUARD_DDB_REGION_KEY
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|region
argument_list|)
argument_list|,
literal|"No DynamoDB region configured"
argument_list|)
expr_stmt|;
comment|// there's no URI here, which complicates life: you cannot
comment|// create AWS providers here which require one.
name|credentials
operator|=
name|createAWSCredentialProviderSet
argument_list|(
literal|null
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|dynamoDB
operator|=
name|createDynamoDB
argument_list|(
name|conf
argument_list|,
name|region
argument_list|,
literal|null
argument_list|,
name|credentials
argument_list|)
expr_stmt|;
name|username
operator|=
name|UserGroupInformation
operator|.
name|getCurrentUser
argument_list|()
operator|.
name|getShortUserName
argument_list|()
expr_stmt|;
comment|// without an executor from the owner FS, create one using
comment|// the executor capacity for work.
name|int
name|executorCapacity
init|=
name|intOption
argument_list|(
name|conf
argument_list|,
name|EXECUTOR_CAPACITY
argument_list|,
name|DEFAULT_EXECUTOR_CAPACITY
argument_list|,
literal|1
argument_list|)
decl_stmt|;
name|executor
operator|=
name|BlockingThreadPoolExecutorService
operator|.
name|newInstance
argument_list|(
name|executorCapacity
argument_list|,
name|executorCapacity
operator|*
literal|2
argument_list|,
name|longOption
argument_list|(
name|conf
argument_list|,
name|KEEPALIVE_TIME
argument_list|,
name|DEFAULT_KEEPALIVE_TIME
argument_list|,
literal|0
argument_list|)
argument_list|,
name|TimeUnit
operator|.
name|SECONDS
argument_list|,
literal|"s3a-ddb-"
operator|+
name|tableName
argument_list|)
expr_stmt|;
name|initDataAccessRetries
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|this
operator|.
name|ttlTimeProvider
operator|=
name|ttlTp
expr_stmt|;
name|tableHandler
operator|=
operator|new
name|DynamoDBMetadataStoreTableManager
argument_list|(
name|dynamoDB
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|amazonDynamoDB
argument_list|,
name|conf
argument_list|,
name|readOp
argument_list|,
name|batchWriteRetryPolicy
argument_list|)
expr_stmt|;
name|this
operator|.
name|table
operator|=
name|tableHandler
operator|.
name|initTable
argument_list|()
expr_stmt|;
block|}
comment|/**    * Set retry policy. This is driven by the value of    * {@link Constants#S3GUARD_DDB_MAX_RETRIES} with an exponential backoff    * between each attempt of {@link Constants#S3GUARD_DDB_THROTTLE_RETRY_INTERVAL}    * milliseconds.    * @param config configuration for data access    */
DECL|method|initDataAccessRetries (Configuration config)
specifier|private
name|void
name|initDataAccessRetries
parameter_list|(
name|Configuration
name|config
parameter_list|)
block|{
name|batchWriteRetryPolicy
operator|=
name|RetryPolicies
operator|.
name|exponentialBackoffRetry
argument_list|(
name|config
operator|.
name|getInt
argument_list|(
name|S3GUARD_DDB_MAX_RETRIES
argument_list|,
name|S3GUARD_DDB_MAX_RETRIES_DEFAULT
argument_list|)
argument_list|,
name|conf
operator|.
name|getTimeDuration
argument_list|(
name|S3GUARD_DDB_THROTTLE_RETRY_INTERVAL
argument_list|,
name|S3GUARD_DDB_THROTTLE_RETRY_INTERVAL_DEFAULT
argument_list|,
name|TimeUnit
operator|.
name|MILLISECONDS
argument_list|)
argument_list|,
name|TimeUnit
operator|.
name|MILLISECONDS
argument_list|)
expr_stmt|;
specifier|final
name|RetryPolicy
name|throttledRetryRetryPolicy
init|=
operator|new
name|S3GuardDataAccessRetryPolicy
argument_list|(
name|config
argument_list|)
decl_stmt|;
name|readOp
operator|=
operator|new
name|Invoker
argument_list|(
name|throttledRetryRetryPolicy
argument_list|,
name|this
operator|::
name|readRetryEvent
argument_list|)
expr_stmt|;
name|writeOp
operator|=
operator|new
name|Invoker
argument_list|(
name|throttledRetryRetryPolicy
argument_list|,
name|this
operator|::
name|writeRetryEvent
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|delete (Path path, final BulkOperationState operationState)
specifier|public
name|void
name|delete
parameter_list|(
name|Path
name|path
parameter_list|,
specifier|final
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
name|innerDelete
argument_list|(
name|path
argument_list|,
literal|true
argument_list|,
name|extractOrCreate
argument_list|(
name|operationState
argument_list|,
name|BulkOperationState
operator|.
name|OperationType
operator|.
name|Delete
argument_list|)
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|forgetMetadata (Path path)
specifier|public
name|void
name|forgetMetadata
parameter_list|(
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Forget metadata for {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|innerDelete
argument_list|(
name|path
argument_list|,
literal|false
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
comment|/**    * Inner delete option, action based on the {@code tombstone} flag.    * No tombstone: delete the entry. Tombstone: create a tombstone entry.    * There is no check as to whether the entry exists in the table first.    * @param path path to delete    * @param tombstone flag to create a tombstone marker    * @param ancestorState ancestor state for context.    * @throws IOException I/O error.    */
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|innerDelete (final Path path, final boolean tombstone, final AncestorState ancestorState)
specifier|private
name|void
name|innerDelete
parameter_list|(
specifier|final
name|Path
name|path
parameter_list|,
specifier|final
name|boolean
name|tombstone
parameter_list|,
specifier|final
name|AncestorState
name|ancestorState
parameter_list|)
throws|throws
name|IOException
block|{
name|checkPath
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Deleting from table {} in region {}: {}"
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|path
argument_list|)
expr_stmt|;
comment|// deleting nonexistent item consumes 1 write capacity; skip it
if|if
condition|(
name|path
operator|.
name|isRoot
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skip deleting root directory as it does not exist in table"
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// the policy on whether repeating delete operations is based
comment|// on that of S3A itself
name|boolean
name|idempotent
init|=
name|S3AFileSystem
operator|.
name|DELETE_CONSIDERED_IDEMPOTENT
decl_stmt|;
if|if
condition|(
name|tombstone
condition|)
block|{
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|ttlTimeProvider
operator|!=
literal|null
argument_list|,
literal|"ttlTimeProvider "
operator|+
literal|"must not be null"
argument_list|)
expr_stmt|;
specifier|final
name|PathMetadata
name|pmTombstone
init|=
name|PathMetadata
operator|.
name|tombstone
argument_list|(
name|path
argument_list|,
name|ttlTimeProvider
operator|.
name|getNow
argument_list|()
argument_list|)
decl_stmt|;
name|Item
name|item
init|=
name|PathMetadataDynamoDBTranslation
operator|.
name|pathMetadataToItem
argument_list|(
operator|new
name|DDBPathMetadata
argument_list|(
name|pmTombstone
argument_list|)
argument_list|)
decl_stmt|;
name|writeOp
operator|.
name|retry
argument_list|(
literal|"Put tombstone"
argument_list|,
name|path
operator|.
name|toString
argument_list|()
argument_list|,
name|idempotent
argument_list|,
parameter_list|()
lambda|->
block|{
name|logPut
argument_list|(
name|ancestorState
argument_list|,
name|item
argument_list|)
expr_stmt|;
name|recordsWritten
argument_list|(
literal|1
argument_list|)
expr_stmt|;
name|table
operator|.
name|putItem
argument_list|(
name|item
argument_list|)
expr_stmt|;
block|}
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|PrimaryKey
name|key
init|=
name|pathToKey
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|writeOp
operator|.
name|retry
argument_list|(
literal|"Delete key"
argument_list|,
name|path
operator|.
name|toString
argument_list|()
argument_list|,
name|idempotent
argument_list|,
parameter_list|()
lambda|->
block|{
comment|// record the attempt so even on retry the counter goes up.
name|logDelete
argument_list|(
name|ancestorState
argument_list|,
name|key
argument_list|)
expr_stmt|;
name|recordsDeleted
argument_list|(
literal|1
argument_list|)
expr_stmt|;
name|table
operator|.
name|deleteItem
argument_list|(
name|key
argument_list|)
expr_stmt|;
block|}
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|deleteSubtree (Path path, final BulkOperationState operationState)
specifier|public
name|void
name|deleteSubtree
parameter_list|(
name|Path
name|path
parameter_list|,
specifier|final
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
name|checkPath
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Deleting subtree from table {} in region {}: {}"
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|path
argument_list|)
expr_stmt|;
specifier|final
name|PathMetadata
name|meta
init|=
name|get
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|meta
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Subtree path {} does not exist; this will be a no-op"
argument_list|,
name|path
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|meta
operator|.
name|isDeleted
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Subtree path {} is deleted; this will be a no-op"
argument_list|,
name|path
argument_list|)
expr_stmt|;
return|return;
block|}
name|deleteEntries
argument_list|(
operator|new
name|InternalIterators
operator|.
name|PathFromRemoteStatusIterator
argument_list|(
operator|new
name|DescendantsIterator
argument_list|(
name|this
argument_list|,
name|meta
argument_list|)
argument_list|)
argument_list|,
name|operationState
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|deletePaths (Collection<Path> paths, final BulkOperationState operationState)
specifier|public
name|void
name|deletePaths
parameter_list|(
name|Collection
argument_list|<
name|Path
argument_list|>
name|paths
parameter_list|,
specifier|final
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
name|deleteEntries
argument_list|(
operator|new
name|InternalIterators
operator|.
name|RemoteIteratorFromIterator
argument_list|<>
argument_list|(
name|paths
operator|.
name|iterator
argument_list|()
argument_list|)
argument_list|,
name|operationState
argument_list|)
expr_stmt|;
block|}
comment|/**    * Delete the entries under an iterator.    * There's no attempt to order the paths: they are    * deleted in the order passed in.    * @param entries entries to delete.    * @param operationState Nullable operation state    * @throws IOException failure    */
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|deleteEntries (RemoteIterator<Path> entries, final BulkOperationState operationState)
specifier|private
name|void
name|deleteEntries
parameter_list|(
name|RemoteIterator
argument_list|<
name|Path
argument_list|>
name|entries
parameter_list|,
specifier|final
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|List
argument_list|<
name|CompletableFuture
argument_list|<
name|Void
argument_list|>
argument_list|>
name|futures
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|AncestorState
name|state
init|=
name|extractOrCreate
argument_list|(
name|operationState
argument_list|,
name|BulkOperationState
operator|.
name|OperationType
operator|.
name|Delete
argument_list|)
decl_stmt|;
while|while
condition|(
name|entries
operator|.
name|hasNext
argument_list|()
condition|)
block|{
specifier|final
name|Path
name|pathToDelete
init|=
name|entries
operator|.
name|next
argument_list|()
decl_stmt|;
name|futures
operator|.
name|add
argument_list|(
name|submit
argument_list|(
name|executor
argument_list|,
parameter_list|()
lambda|->
block|{
name|innerDelete
argument_list|(
name|pathToDelete
argument_list|,
literal|true
argument_list|,
name|state
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|futures
operator|.
name|size
argument_list|()
operator|>
name|S3GUARD_DDB_SUBMITTED_TASK_LIMIT
condition|)
block|{
comment|// first batch done; block for completion.
name|waitForCompletion
argument_list|(
name|futures
argument_list|)
expr_stmt|;
name|futures
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
comment|// now wait for the final set.
name|waitForCompletion
argument_list|(
name|futures
argument_list|)
expr_stmt|;
block|}
comment|/**    * Get a consistent view of an item.    * @param path path to look up in the database    * @return the result    * @throws IOException failure    */
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|getConsistentItem (final Path path)
specifier|private
name|Item
name|getConsistentItem
parameter_list|(
specifier|final
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
name|PrimaryKey
name|key
init|=
name|pathToKey
argument_list|(
name|path
argument_list|)
decl_stmt|;
specifier|final
name|GetItemSpec
name|spec
init|=
operator|new
name|GetItemSpec
argument_list|()
operator|.
name|withPrimaryKey
argument_list|(
name|key
argument_list|)
operator|.
name|withConsistentRead
argument_list|(
literal|true
argument_list|)
decl_stmt|;
comment|// strictly consistent read
return|return
name|readOp
operator|.
name|retry
argument_list|(
literal|"get"
argument_list|,
name|path
operator|.
name|toString
argument_list|()
argument_list|,
literal|true
argument_list|,
parameter_list|()
lambda|->
block|{
name|recordsRead
argument_list|(
literal|1
argument_list|)
expr_stmt|;
return|return
name|table
operator|.
name|getItem
argument_list|(
name|spec
argument_list|)
return|;
block|}
argument_list|)
return|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|get (Path path)
specifier|public
name|DDBPathMetadata
name|get
parameter_list|(
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|get
argument_list|(
name|path
argument_list|,
literal|false
argument_list|)
return|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|get (Path path, boolean wantEmptyDirectoryFlag)
specifier|public
name|DDBPathMetadata
name|get
parameter_list|(
name|Path
name|path
parameter_list|,
name|boolean
name|wantEmptyDirectoryFlag
parameter_list|)
throws|throws
name|IOException
block|{
name|checkPath
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Get from table {} in region {}: {}. wantEmptyDirectory={}"
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|path
argument_list|,
name|wantEmptyDirectoryFlag
argument_list|)
expr_stmt|;
name|DDBPathMetadata
name|result
init|=
name|innerGet
argument_list|(
name|path
argument_list|,
name|wantEmptyDirectoryFlag
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"result of get {} is: {}"
argument_list|,
name|path
argument_list|,
name|result
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
comment|/**    * Inner get operation, as invoked in the retry logic.    * @param path the path to get    * @param wantEmptyDirectoryFlag Set to true to give a hint to the    *   MetadataStore that it should try to compute the empty directory flag.    * @return metadata for {@code path}, {@code null} if not found    * @throws IOException IO problem    */
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|innerGet (Path path, boolean wantEmptyDirectoryFlag)
specifier|private
name|DDBPathMetadata
name|innerGet
parameter_list|(
name|Path
name|path
parameter_list|,
name|boolean
name|wantEmptyDirectoryFlag
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|DDBPathMetadata
name|meta
decl_stmt|;
if|if
condition|(
name|path
operator|.
name|isRoot
argument_list|()
condition|)
block|{
comment|// Root does not persist in the table
name|meta
operator|=
operator|new
name|DDBPathMetadata
argument_list|(
name|makeDirStatus
argument_list|(
name|username
argument_list|,
name|path
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
specifier|final
name|Item
name|item
init|=
name|getConsistentItem
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|meta
operator|=
name|itemToPathMetadata
argument_list|(
name|item
argument_list|,
name|username
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Get from table {} in region {} returning for {}: {}"
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|path
argument_list|,
name|meta
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|wantEmptyDirectoryFlag
operator|&&
name|meta
operator|!=
literal|null
condition|)
block|{
specifier|final
name|FileStatus
name|status
init|=
name|meta
operator|.
name|getFileStatus
argument_list|()
decl_stmt|;
comment|// for directory, we query its direct children to determine isEmpty bit
if|if
condition|(
name|status
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
specifier|final
name|QuerySpec
name|spec
init|=
operator|new
name|QuerySpec
argument_list|()
operator|.
name|withHashKey
argument_list|(
name|pathToParentKeyAttribute
argument_list|(
name|path
argument_list|)
argument_list|)
operator|.
name|withConsistentRead
argument_list|(
literal|true
argument_list|)
operator|.
name|withFilterExpression
argument_list|(
name|IS_DELETED
operator|+
literal|" = :false"
argument_list|)
operator|.
name|withValueMap
argument_list|(
name|DELETE_TRACKING_VALUE_MAP
argument_list|)
decl_stmt|;
name|boolean
name|hasChildren
init|=
name|readOp
operator|.
name|retry
argument_list|(
literal|"get/hasChildren"
argument_list|,
name|path
operator|.
name|toString
argument_list|()
argument_list|,
literal|true
argument_list|,
parameter_list|()
lambda|->
name|table
operator|.
name|query
argument_list|(
name|spec
argument_list|)
operator|.
name|iterator
argument_list|()
operator|.
name|hasNext
argument_list|()
argument_list|)
decl_stmt|;
comment|// If directory is authoritative, we can set the empty directory flag
comment|// to TRUE or FALSE. Otherwise FALSE, or UNKNOWN.
if|if
condition|(
name|meta
operator|.
name|isAuthoritativeDir
argument_list|()
condition|)
block|{
name|meta
operator|.
name|setIsEmptyDirectory
argument_list|(
name|hasChildren
condition|?
name|Tristate
operator|.
name|FALSE
else|:
name|Tristate
operator|.
name|TRUE
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|meta
operator|.
name|setIsEmptyDirectory
argument_list|(
name|hasChildren
condition|?
name|Tristate
operator|.
name|FALSE
else|:
name|Tristate
operator|.
name|UNKNOWN
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|meta
return|;
block|}
comment|/**    * Make a S3AFileStatus object for a directory at given path.    * The FileStatus only contains what S3A needs, and omits mod time    * since S3A uses its own implementation which returns current system time.    * @param dirOwner  username of owner    * @param path   path to dir    * @return new S3AFileStatus    */
DECL|method|makeDirStatus (String dirOwner, Path path)
specifier|private
name|S3AFileStatus
name|makeDirStatus
parameter_list|(
name|String
name|dirOwner
parameter_list|,
name|Path
name|path
parameter_list|)
block|{
return|return
operator|new
name|S3AFileStatus
argument_list|(
name|Tristate
operator|.
name|UNKNOWN
argument_list|,
name|path
argument_list|,
name|dirOwner
argument_list|)
return|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|listChildren (final Path path)
specifier|public
name|DirListingMetadata
name|listChildren
parameter_list|(
specifier|final
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
name|checkPath
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Listing table {} in region {}: {}"
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|path
argument_list|)
expr_stmt|;
comment|// find the children in the table
return|return
name|readOp
operator|.
name|retry
argument_list|(
literal|"listChildren"
argument_list|,
name|path
operator|.
name|toString
argument_list|()
argument_list|,
literal|true
argument_list|,
parameter_list|()
lambda|->
block|{
specifier|final
name|QuerySpec
name|spec
init|=
operator|new
name|QuerySpec
argument_list|()
operator|.
name|withHashKey
argument_list|(
name|pathToParentKeyAttribute
argument_list|(
name|path
argument_list|)
argument_list|)
operator|.
name|withConsistentRead
argument_list|(
literal|true
argument_list|)
decl_stmt|;
comment|// strictly consistent read
specifier|final
name|ItemCollection
argument_list|<
name|QueryOutcome
argument_list|>
name|items
init|=
name|table
operator|.
name|query
argument_list|(
name|spec
argument_list|)
decl_stmt|;
specifier|final
name|List
argument_list|<
name|PathMetadata
argument_list|>
name|metas
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|Item
name|item
range|:
name|items
control|)
block|{
name|DDBPathMetadata
name|meta
init|=
name|itemToPathMetadata
argument_list|(
name|item
argument_list|,
name|username
argument_list|)
decl_stmt|;
name|metas
operator|.
name|add
argument_list|(
name|meta
argument_list|)
expr_stmt|;
block|}
comment|// Minor race condition here - if the path is deleted between
comment|// getting the list of items and the directory metadata we might
comment|// get a null in DDBPathMetadata.
name|DDBPathMetadata
name|dirPathMeta
init|=
name|get
argument_list|(
name|path
argument_list|)
decl_stmt|;
comment|// Filter expired entries.
specifier|final
name|DirListingMetadata
name|dirListing
init|=
name|getDirListingMetadataFromDirMetaAndList
argument_list|(
name|path
argument_list|,
name|metas
argument_list|,
name|dirPathMeta
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirListing
operator|!=
literal|null
condition|)
block|{
name|dirListing
operator|.
name|removeExpiredEntriesFromListing
argument_list|(
name|ttlTimeProvider
operator|.
name|getMetadataTtl
argument_list|()
argument_list|,
name|ttlTimeProvider
operator|.
name|getNow
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|dirListing
return|;
block|}
argument_list|)
return|;
block|}
DECL|method|getDirListingMetadataFromDirMetaAndList (Path path, List<PathMetadata> metas, DDBPathMetadata dirPathMeta)
name|DirListingMetadata
name|getDirListingMetadataFromDirMetaAndList
parameter_list|(
name|Path
name|path
parameter_list|,
name|List
argument_list|<
name|PathMetadata
argument_list|>
name|metas
parameter_list|,
name|DDBPathMetadata
name|dirPathMeta
parameter_list|)
block|{
name|boolean
name|isAuthoritative
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|dirPathMeta
operator|!=
literal|null
condition|)
block|{
name|isAuthoritative
operator|=
name|dirPathMeta
operator|.
name|isAuthoritativeDir
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|trace
argument_list|(
literal|"Listing table {} in region {} for {} returning {}"
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|path
argument_list|,
name|metas
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|metas
operator|.
name|isEmpty
argument_list|()
operator|&&
name|dirPathMeta
operator|==
literal|null
condition|)
block|{
comment|// We handle this case as the directory is deleted.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Directory marker is deleted, but the list of the directory "
operator|+
literal|"elements is not empty: {}. This case is handled as if the "
operator|+
literal|"directory was deleted."
argument_list|,
name|metas
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
if|if
condition|(
name|metas
operator|.
name|isEmpty
argument_list|()
operator|&&
name|dirPathMeta
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
operator|new
name|DirListingMetadata
argument_list|(
name|path
argument_list|,
name|metas
argument_list|,
name|isAuthoritative
argument_list|,
name|dirPathMeta
operator|.
name|getLastUpdated
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Build the list of all parent entries.    *<p>    *<b>Thread safety:</b> none. Callers must synchronize access.    *<p>    * Callers are required to synchronize on ancestorState.    * @param pathsToCreate paths to create    * @param ancestorState ongoing ancestor state.    * @return the full ancestry paths    */
DECL|method|completeAncestry ( final Collection<DDBPathMetadata> pathsToCreate, final AncestorState ancestorState)
specifier|private
name|Collection
argument_list|<
name|DDBPathMetadata
argument_list|>
name|completeAncestry
parameter_list|(
specifier|final
name|Collection
argument_list|<
name|DDBPathMetadata
argument_list|>
name|pathsToCreate
parameter_list|,
specifier|final
name|AncestorState
name|ancestorState
parameter_list|)
throws|throws
name|PathIOException
block|{
comment|// Key on path to allow fast lookup
name|Map
argument_list|<
name|Path
argument_list|,
name|DDBPathMetadata
argument_list|>
name|ancestry
init|=
operator|new
name|HashMap
argument_list|<>
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Completing ancestry for {} paths"
argument_list|,
name|pathsToCreate
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
comment|// we sort the inputs to guarantee that the topmost entries come first.
comment|// that way if the put request contains both parents and children
comment|// then the existing parents will not be re-created -they will just
comment|// be added to the ancestor list first.
name|List
argument_list|<
name|DDBPathMetadata
argument_list|>
name|sortedPaths
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|pathsToCreate
argument_list|)
decl_stmt|;
name|sortedPaths
operator|.
name|sort
argument_list|(
name|PathOrderComparators
operator|.
name|TOPMOST_PM_FIRST
argument_list|)
expr_stmt|;
comment|// iterate through the paths.
for|for
control|(
name|DDBPathMetadata
name|meta
range|:
name|sortedPaths
control|)
block|{
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|meta
operator|!=
literal|null
argument_list|)
expr_stmt|;
name|Path
name|path
init|=
name|meta
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Adding entry {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
if|if
condition|(
name|path
operator|.
name|isRoot
argument_list|()
condition|)
block|{
comment|// this is a root entry: do not add it.
break|break;
block|}
comment|// create the new entry
name|DDBPathMetadata
name|entry
init|=
operator|new
name|DDBPathMetadata
argument_list|(
name|meta
argument_list|)
decl_stmt|;
comment|// add it to the ancestor state, failing if it is already there and
comment|// of a different type.
name|DDBPathMetadata
name|oldEntry
init|=
name|ancestorState
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|entry
argument_list|)
decl_stmt|;
if|if
condition|(
name|oldEntry
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
operator|!
name|oldEntry
operator|.
name|getFileStatus
argument_list|()
operator|.
name|isDirectory
argument_list|()
operator|||
operator|!
name|entry
operator|.
name|getFileStatus
argument_list|()
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
comment|// check for and warn if the existing bulk operation overwrote it.
comment|// this should never occur outside tests explicitly creating it
name|LOG
operator|.
name|warn
argument_list|(
literal|"Overwriting a S3Guard file created in the operation: {}"
argument_list|,
name|oldEntry
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"With new entry: {}"
argument_list|,
name|entry
argument_list|)
expr_stmt|;
comment|// restore the old state
name|ancestorState
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|oldEntry
argument_list|)
expr_stmt|;
comment|// then raise an exception
throw|throw
operator|new
name|PathIOException
argument_list|(
name|path
operator|.
name|toString
argument_list|()
argument_list|,
name|E_INCONSISTENT_UPDATE
argument_list|)
throw|;
block|}
else|else
block|{
comment|// a directory is already present. Log and continue.
name|LOG
operator|.
name|debug
argument_list|(
literal|"Directory at {} being updated with value {}"
argument_list|,
name|path
argument_list|,
name|entry
argument_list|)
expr_stmt|;
block|}
block|}
name|ancestry
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|entry
argument_list|)
expr_stmt|;
name|Path
name|parent
init|=
name|path
operator|.
name|getParent
argument_list|()
decl_stmt|;
while|while
condition|(
operator|!
name|parent
operator|.
name|isRoot
argument_list|()
operator|&&
operator|!
name|ancestry
operator|.
name|containsKey
argument_list|(
name|parent
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|ancestorState
operator|.
name|findEntry
argument_list|(
name|parent
argument_list|,
literal|true
argument_list|)
condition|)
block|{
comment|// don't add this entry, but carry on with the parents
name|LOG
operator|.
name|debug
argument_list|(
literal|"auto-create ancestor path {} for child path {}"
argument_list|,
name|parent
argument_list|,
name|path
argument_list|)
expr_stmt|;
specifier|final
name|S3AFileStatus
name|status
init|=
name|makeDirStatus
argument_list|(
name|parent
argument_list|,
name|username
argument_list|)
decl_stmt|;
name|DDBPathMetadata
name|md
init|=
operator|new
name|DDBPathMetadata
argument_list|(
name|status
argument_list|,
name|Tristate
operator|.
name|FALSE
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|,
name|ttlTimeProvider
operator|.
name|getNow
argument_list|()
argument_list|)
decl_stmt|;
name|ancestorState
operator|.
name|put
argument_list|(
name|parent
argument_list|,
name|md
argument_list|)
expr_stmt|;
name|ancestry
operator|.
name|put
argument_list|(
name|parent
argument_list|,
name|md
argument_list|)
expr_stmt|;
block|}
name|parent
operator|=
name|parent
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|ancestry
operator|.
name|values
argument_list|()
return|;
block|}
comment|/**    * {@inheritDoc}    *<p>    * The implementation scans all up the directory tree and does a get()    * for each entry; at each level one is found it is added to the ancestor    * state.    *<p>    * The original implementation would stop on finding the first non-empty    * parent. This (re) implementation issues a GET for every parent entry    * and so detects and recovers from a tombstone marker further up the tree    * (i.e. an inconsistent store is corrected for).    *<p>    * if {@code operationState} is not null, when this method returns the    * operation state will be updated with all new entries created.    * This ensures that subsequent operations with the same store will not    * trigger new updates.    * @param qualifiedPath path to update    * @param operationState (nullable) operational state for a bulk update    * @throws IOException on failure.    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"SynchronizationOnLocalVariableOrMethodParameter"
argument_list|)
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|addAncestors (final Path qualifiedPath, @Nullable final BulkOperationState operationState)
specifier|public
name|void
name|addAncestors
parameter_list|(
specifier|final
name|Path
name|qualifiedPath
parameter_list|,
annotation|@
name|Nullable
specifier|final
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
name|Collection
argument_list|<
name|DDBPathMetadata
argument_list|>
name|newDirs
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
specifier|final
name|AncestorState
name|ancestorState
init|=
name|extractOrCreate
argument_list|(
name|operationState
argument_list|,
name|BulkOperationState
operator|.
name|OperationType
operator|.
name|Rename
argument_list|)
decl_stmt|;
name|Path
name|parent
init|=
name|qualifiedPath
operator|.
name|getParent
argument_list|()
decl_stmt|;
name|boolean
name|entryFound
init|=
literal|false
decl_stmt|;
comment|// Iterate up the parents.
comment|// note that only ancestorState get/set operations are synchronized;
comment|// the DDB read between them is not. As a result, more than one
comment|// thread may probe the state, find the entry missing, do the database
comment|// query and add the entry.
comment|// This is done to avoid making the remote dynamo query part of the
comment|// synchronized block.
comment|// If a race does occur, the cost is simply one extra GET and potentially
comment|// one extra PUT.
while|while
condition|(
operator|!
name|parent
operator|.
name|isRoot
argument_list|()
condition|)
block|{
synchronized|synchronized
init|(
name|ancestorState
init|)
block|{
if|if
condition|(
name|ancestorState
operator|.
name|contains
argument_list|(
name|parent
argument_list|)
condition|)
block|{
comment|// the ancestry map contains the key, so no need to even look for it.
break|break;
block|}
block|}
comment|// we don't worry about tombstone expiry here as expired or not,
comment|// a directory entry will go in.
name|PathMetadata
name|directory
init|=
name|get
argument_list|(
name|parent
argument_list|)
decl_stmt|;
if|if
condition|(
name|directory
operator|==
literal|null
operator|||
name|directory
operator|.
name|isDeleted
argument_list|()
condition|)
block|{
if|if
condition|(
name|entryFound
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Inconsistent S3Guard table: adding directory {}"
argument_list|,
name|parent
argument_list|)
expr_stmt|;
block|}
name|S3AFileStatus
name|status
init|=
name|makeDirStatus
argument_list|(
name|username
argument_list|,
name|parent
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Adding new ancestor entry {}"
argument_list|,
name|status
argument_list|)
expr_stmt|;
name|DDBPathMetadata
name|meta
init|=
operator|new
name|DDBPathMetadata
argument_list|(
name|status
argument_list|,
name|Tristate
operator|.
name|FALSE
argument_list|,
literal|false
argument_list|,
name|ttlTimeProvider
operator|.
name|getNow
argument_list|()
argument_list|)
decl_stmt|;
name|newDirs
operator|.
name|add
argument_list|(
name|meta
argument_list|)
expr_stmt|;
comment|// Do not update ancestor state here, as it
comment|// will happen in the innerPut() call. Were we to add it
comment|// here that put operation would actually (mistakenly) skip
comment|// creating the entry.
block|}
else|else
block|{
comment|// an entry was found. Check its type
name|entryFound
operator|=
literal|true
expr_stmt|;
if|if
condition|(
name|directory
operator|.
name|getFileStatus
argument_list|()
operator|.
name|isFile
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|PathIOException
argument_list|(
name|parent
operator|.
name|toString
argument_list|()
argument_list|,
literal|"Cannot overwrite parent file: metastore is"
operator|+
literal|" in an inconsistent state"
argument_list|)
throw|;
block|}
comment|// the directory exists. Add it to the ancestor state for next time.
synchronized|synchronized
init|(
name|ancestorState
init|)
block|{
name|ancestorState
operator|.
name|put
argument_list|(
name|parent
argument_list|,
operator|new
name|DDBPathMetadata
argument_list|(
name|directory
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|parent
operator|=
name|parent
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
comment|// the listing of directories to put is all those parents which we know
comment|// are not in the store or BulkOperationState.
if|if
condition|(
operator|!
name|newDirs
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// patch up the time.
name|patchLastUpdated
argument_list|(
name|newDirs
argument_list|,
name|ttlTimeProvider
argument_list|)
expr_stmt|;
name|innerPut
argument_list|(
name|newDirs
argument_list|,
name|operationState
argument_list|,
name|ttlTimeProvider
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * {@inheritDoc}.    *    * The DDB implementation sorts all the paths such that new items    * are ordered highest level entry first; deleted items are ordered    * lowest entry first.    *    * This is to ensure that if a client failed partway through the update,    * there will no entries in the table which lack parent entries.    * @param pathsToDelete Collection of all paths that were removed from the    *                      source directory tree of the move.    * @param pathsToCreate Collection of all PathMetadata for the new paths    *                      that were created at the destination of the rename    *                      ().    * @param operationState Any ongoing state supplied to the rename tracker    *                      which is to be passed in with each move operation.    * @throws IOException if there is an error    */
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|move (@ullable Collection<Path> pathsToDelete, @Nullable Collection<PathMetadata> pathsToCreate, @Nullable final BulkOperationState operationState)
specifier|public
name|void
name|move
parameter_list|(
annotation|@
name|Nullable
name|Collection
argument_list|<
name|Path
argument_list|>
name|pathsToDelete
parameter_list|,
annotation|@
name|Nullable
name|Collection
argument_list|<
name|PathMetadata
argument_list|>
name|pathsToCreate
parameter_list|,
annotation|@
name|Nullable
specifier|final
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|pathsToDelete
operator|==
literal|null
operator|&&
name|pathsToCreate
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Moving paths of table {} in region {}: {} paths to delete and {}"
operator|+
literal|" paths to create"
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|pathsToDelete
operator|==
literal|null
condition|?
literal|0
else|:
name|pathsToDelete
operator|.
name|size
argument_list|()
argument_list|,
name|pathsToCreate
operator|==
literal|null
condition|?
literal|0
else|:
name|pathsToCreate
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|trace
argument_list|(
literal|"move: pathsToDelete = {}, pathsToCreate = {}"
argument_list|,
name|pathsToDelete
argument_list|,
name|pathsToCreate
argument_list|)
expr_stmt|;
comment|// In DynamoDBMetadataStore implementation, we assume that if a path
comment|// exists, all its ancestors will also exist in the table.
comment|// Following code is to maintain this invariant by putting all ancestor
comment|// directories of the paths to create.
comment|// ancestor paths that are not explicitly added to paths to create
name|AncestorState
name|ancestorState
init|=
name|extractOrCreate
argument_list|(
name|operationState
argument_list|,
name|BulkOperationState
operator|.
name|OperationType
operator|.
name|Rename
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|DDBPathMetadata
argument_list|>
name|newItems
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
if|if
condition|(
name|pathsToCreate
operator|!=
literal|null
condition|)
block|{
comment|// create all parent entries.
comment|// this is synchronized on the move state so that across both serialized
comment|// and parallelized renames, duplicate ancestor entries are not created.
synchronized|synchronized
init|(
name|ancestorState
init|)
block|{
name|newItems
operator|.
name|addAll
argument_list|(
name|completeAncestry
argument_list|(
name|pathMetaToDDBPathMeta
argument_list|(
name|pathsToCreate
argument_list|)
argument_list|,
name|ancestorState
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
comment|// sort all the new items topmost first.
name|newItems
operator|.
name|sort
argument_list|(
name|PathOrderComparators
operator|.
name|TOPMOST_PM_FIRST
argument_list|)
expr_stmt|;
comment|// now process the deletions.
if|if
condition|(
name|pathsToDelete
operator|!=
literal|null
condition|)
block|{
name|List
argument_list|<
name|DDBPathMetadata
argument_list|>
name|tombstones
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|pathsToDelete
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|meta
range|:
name|pathsToDelete
control|)
block|{
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|ttlTimeProvider
operator|!=
literal|null
argument_list|,
literal|"ttlTimeProvider"
operator|+
literal|" must not be null"
argument_list|)
expr_stmt|;
specifier|final
name|PathMetadata
name|pmTombstone
init|=
name|PathMetadata
operator|.
name|tombstone
argument_list|(
name|meta
argument_list|,
name|ttlTimeProvider
operator|.
name|getNow
argument_list|()
argument_list|)
decl_stmt|;
name|tombstones
operator|.
name|add
argument_list|(
operator|new
name|DDBPathMetadata
argument_list|(
name|pmTombstone
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// sort all the tombstones lowest first.
name|tombstones
operator|.
name|sort
argument_list|(
name|PathOrderComparators
operator|.
name|TOPMOST_PM_LAST
argument_list|)
expr_stmt|;
name|newItems
operator|.
name|addAll
argument_list|(
name|tombstones
argument_list|)
expr_stmt|;
block|}
name|processBatchWriteRequest
argument_list|(
name|ancestorState
argument_list|,
literal|null
argument_list|,
name|pathMetadataToItem
argument_list|(
name|newItems
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**    * Helper method to issue a batch write request to DynamoDB.    *<ol>    *<li>Keys to delete are processed ahead of writing new items.</li>    *<li>No attempt is made to sort the input: the caller must do that</li>    *</ol>    * As well as retrying on the operation invocation, incomplete    * batches are retried until all have been processed.    *    * @param ancestorState ancestor state for logging    * @param keysToDelete primary keys to be deleted; can be null    * @param itemsToPut new items to be put; can be null    * @return the number of iterations needed to complete the call.    */
annotation|@
name|Retries
operator|.
name|RetryTranslated
argument_list|(
literal|"Outstanding batch items are updated with backoff"
argument_list|)
DECL|method|processBatchWriteRequest ( @ullable AncestorState ancestorState, PrimaryKey[] keysToDelete, Item[] itemsToPut)
specifier|private
name|int
name|processBatchWriteRequest
parameter_list|(
annotation|@
name|Nullable
name|AncestorState
name|ancestorState
parameter_list|,
name|PrimaryKey
index|[]
name|keysToDelete
parameter_list|,
name|Item
index|[]
name|itemsToPut
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|int
name|totalToDelete
init|=
operator|(
name|keysToDelete
operator|==
literal|null
condition|?
literal|0
else|:
name|keysToDelete
operator|.
name|length
operator|)
decl_stmt|;
specifier|final
name|int
name|totalToPut
init|=
operator|(
name|itemsToPut
operator|==
literal|null
condition|?
literal|0
else|:
name|itemsToPut
operator|.
name|length
operator|)
decl_stmt|;
if|if
condition|(
name|totalToPut
operator|==
literal|0
operator|&&
name|totalToDelete
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Ignoring empty batch write request"
argument_list|)
expr_stmt|;
return|return
literal|0
return|;
block|}
name|int
name|count
init|=
literal|0
decl_stmt|;
name|int
name|batches
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|count
operator|<
name|totalToDelete
operator|+
name|totalToPut
condition|)
block|{
specifier|final
name|TableWriteItems
name|writeItems
init|=
operator|new
name|TableWriteItems
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|int
name|numToDelete
init|=
literal|0
decl_stmt|;
if|if
condition|(
name|keysToDelete
operator|!=
literal|null
operator|&&
name|count
operator|<
name|totalToDelete
condition|)
block|{
name|numToDelete
operator|=
name|Math
operator|.
name|min
argument_list|(
name|S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT
argument_list|,
name|totalToDelete
operator|-
name|count
argument_list|)
expr_stmt|;
name|PrimaryKey
index|[]
name|toDelete
init|=
name|Arrays
operator|.
name|copyOfRange
argument_list|(
name|keysToDelete
argument_list|,
name|count
argument_list|,
name|count
operator|+
name|numToDelete
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Deleting {} entries: {}"
argument_list|,
name|toDelete
operator|.
name|length
argument_list|,
name|toDelete
argument_list|)
expr_stmt|;
name|writeItems
operator|.
name|withPrimaryKeysToDelete
argument_list|(
name|toDelete
argument_list|)
expr_stmt|;
name|count
operator|+=
name|numToDelete
expr_stmt|;
block|}
if|if
condition|(
name|numToDelete
operator|<
name|S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT
operator|&&
name|itemsToPut
operator|!=
literal|null
operator|&&
name|count
operator|<
name|totalToDelete
operator|+
name|totalToPut
condition|)
block|{
specifier|final
name|int
name|numToPut
init|=
name|Math
operator|.
name|min
argument_list|(
name|S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT
operator|-
name|numToDelete
argument_list|,
name|totalToDelete
operator|+
name|totalToPut
operator|-
name|count
argument_list|)
decl_stmt|;
specifier|final
name|int
name|index
init|=
name|count
operator|-
name|totalToDelete
decl_stmt|;
name|writeItems
operator|.
name|withItemsToPut
argument_list|(
name|Arrays
operator|.
name|copyOfRange
argument_list|(
name|itemsToPut
argument_list|,
name|index
argument_list|,
name|index
operator|+
name|numToPut
argument_list|)
argument_list|)
expr_stmt|;
name|count
operator|+=
name|numToPut
expr_stmt|;
block|}
comment|// if there's a retry and another process updates things then it's not
comment|// quite idempotent, but this was the case anyway
name|batches
operator|++
expr_stmt|;
name|BatchWriteItemOutcome
name|res
init|=
name|writeOp
operator|.
name|retry
argument_list|(
literal|"batch write"
argument_list|,
literal|""
argument_list|,
literal|true
argument_list|,
parameter_list|()
lambda|->
name|dynamoDB
operator|.
name|batchWriteItem
argument_list|(
name|writeItems
argument_list|)
argument_list|)
decl_stmt|;
comment|// Check for unprocessed keys in case of exceeding provisioned throughput
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|WriteRequest
argument_list|>
argument_list|>
name|unprocessed
init|=
name|res
operator|.
name|getUnprocessedItems
argument_list|()
decl_stmt|;
name|int
name|retryCount
init|=
literal|0
decl_stmt|;
while|while
condition|(
operator|!
name|unprocessed
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|batchWriteCapacityExceededEvents
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
name|batches
operator|++
expr_stmt|;
name|retryBackoffOnBatchWrite
argument_list|(
name|retryCount
operator|++
argument_list|)
expr_stmt|;
comment|// use a different reference to keep the compiler quiet
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|WriteRequest
argument_list|>
argument_list|>
name|upx
init|=
name|unprocessed
decl_stmt|;
name|res
operator|=
name|writeOp
operator|.
name|retry
argument_list|(
literal|"batch write"
argument_list|,
literal|""
argument_list|,
literal|true
argument_list|,
parameter_list|()
lambda|->
name|dynamoDB
operator|.
name|batchWriteItemUnprocessed
argument_list|(
name|upx
argument_list|)
argument_list|)
expr_stmt|;
name|unprocessed
operator|=
name|res
operator|.
name|getUnprocessedItems
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|itemsToPut
operator|!=
literal|null
condition|)
block|{
name|recordsWritten
argument_list|(
name|itemsToPut
operator|.
name|length
argument_list|)
expr_stmt|;
name|logPut
argument_list|(
name|ancestorState
argument_list|,
name|itemsToPut
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|keysToDelete
operator|!=
literal|null
condition|)
block|{
name|recordsDeleted
argument_list|(
name|keysToDelete
operator|.
name|length
argument_list|)
expr_stmt|;
name|logDelete
argument_list|(
name|ancestorState
argument_list|,
name|keysToDelete
argument_list|)
expr_stmt|;
block|}
return|return
name|batches
return|;
block|}
comment|/**    * Put the current thread to sleep to implement exponential backoff    * depending on retryCount.  If max retries are exceeded, throws an    * exception instead.    *    * @param retryCount number of retries so far    * @throws IOException when max retryCount is exceeded.    */
DECL|method|retryBackoffOnBatchWrite (int retryCount)
specifier|private
name|void
name|retryBackoffOnBatchWrite
parameter_list|(
name|int
name|retryCount
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
comment|// Our RetryPolicy ignores everything but retryCount here.
name|RetryPolicy
operator|.
name|RetryAction
name|action
init|=
name|batchWriteRetryPolicy
operator|.
name|shouldRetry
argument_list|(
literal|null
argument_list|,
name|retryCount
argument_list|,
literal|0
argument_list|,
literal|true
argument_list|)
decl_stmt|;
if|if
condition|(
name|action
operator|.
name|action
operator|==
name|RetryPolicy
operator|.
name|RetryAction
operator|.
name|RetryDecision
operator|.
name|FAIL
condition|)
block|{
comment|// Create an AWSServiceThrottledException, with a fake inner cause
comment|// which we fill in to look like a real exception so
comment|// error messages look sensible
name|AmazonServiceException
name|cause
init|=
operator|new
name|AmazonServiceException
argument_list|(
literal|"Throttling"
argument_list|)
decl_stmt|;
name|cause
operator|.
name|setServiceName
argument_list|(
literal|"S3Guard"
argument_list|)
expr_stmt|;
name|cause
operator|.
name|setStatusCode
argument_list|(
name|AWSServiceThrottledException
operator|.
name|STATUS_CODE
argument_list|)
expr_stmt|;
name|cause
operator|.
name|setErrorCode
argument_list|(
name|THROTTLING
argument_list|)
expr_stmt|;
comment|// used in real AWS errors
name|cause
operator|.
name|setErrorType
argument_list|(
name|AmazonServiceException
operator|.
name|ErrorType
operator|.
name|Service
argument_list|)
expr_stmt|;
name|cause
operator|.
name|setErrorMessage
argument_list|(
name|THROTTLING
argument_list|)
expr_stmt|;
name|cause
operator|.
name|setRequestId
argument_list|(
literal|"n/a"
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|AWSServiceThrottledException
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Max retries during batch write exceeded"
operator|+
literal|" (%d) for DynamoDB."
operator|+
name|HINT_DDB_IOPS_TOO_LOW
argument_list|,
name|retryCount
argument_list|)
argument_list|,
name|cause
argument_list|)
throw|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Sleeping {} msec before next retry"
argument_list|,
name|action
operator|.
name|delayMillis
argument_list|)
expr_stmt|;
name|Thread
operator|.
name|sleep
argument_list|(
name|action
operator|.
name|delayMillis
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
throw|throw
operator|(
name|IOException
operator|)
operator|new
name|InterruptedIOException
argument_list|(
name|e
operator|.
name|toString
argument_list|()
argument_list|)
operator|.
name|initCause
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
name|e
throw|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unexpected exception "
operator|+
name|e
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|put (final PathMetadata meta)
specifier|public
name|void
name|put
parameter_list|(
specifier|final
name|PathMetadata
name|meta
parameter_list|)
throws|throws
name|IOException
block|{
name|put
argument_list|(
name|meta
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|put ( final PathMetadata meta, @Nullable final BulkOperationState operationState)
specifier|public
name|void
name|put
parameter_list|(
specifier|final
name|PathMetadata
name|meta
parameter_list|,
annotation|@
name|Nullable
specifier|final
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
comment|// For a deeply nested path, this method will automatically create the full
comment|// ancestry and save respective item in DynamoDB table.
comment|// So after put operation, we maintain the invariant that if a path exists,
comment|// all its ancestors will also exist in the table.
comment|// For performance purpose, we generate the full paths to put and use batch
comment|// write item request to save the items.
name|LOG
operator|.
name|debug
argument_list|(
literal|"Saving to table {} in region {}: {}"
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|meta
argument_list|)
expr_stmt|;
name|Collection
argument_list|<
name|PathMetadata
argument_list|>
name|wrapper
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
literal|1
argument_list|)
decl_stmt|;
name|wrapper
operator|.
name|add
argument_list|(
name|meta
argument_list|)
expr_stmt|;
name|put
argument_list|(
name|wrapper
argument_list|,
name|operationState
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|put ( final Collection<? extends PathMetadata> metas, @Nullable final BulkOperationState operationState)
specifier|public
name|void
name|put
parameter_list|(
specifier|final
name|Collection
argument_list|<
name|?
extends|extends
name|PathMetadata
argument_list|>
name|metas
parameter_list|,
annotation|@
name|Nullable
specifier|final
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
name|innerPut
argument_list|(
name|pathMetaToDDBPathMeta
argument_list|(
name|metas
argument_list|)
argument_list|,
name|operationState
argument_list|,
name|ttlTimeProvider
argument_list|)
expr_stmt|;
block|}
comment|/**    * Internal put operation.    *<p>    * The ancestors to all entries are added to the set of entries to write,    * provided they are not already stored in any supplied operation state.    * Both the supplied metadata entries and ancestor entries are sorted    * so that the topmost entries are written first.    * This is to ensure that a failure partway through the operation will not    * create entries in the table without parents.    * @param metas metadata entries to write.    * @param operationState (nullable) operational state for a bulk update    * @param ttlTp The time provider for metadata expiry    * @throws IOException failure.    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"SynchronizationOnLocalVariableOrMethodParameter"
argument_list|)
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|innerPut ( final Collection<DDBPathMetadata> metas, @Nullable final BulkOperationState operationState, final ITtlTimeProvider ttlTp)
specifier|private
name|void
name|innerPut
parameter_list|(
specifier|final
name|Collection
argument_list|<
name|DDBPathMetadata
argument_list|>
name|metas
parameter_list|,
annotation|@
name|Nullable
specifier|final
name|BulkOperationState
name|operationState
parameter_list|,
specifier|final
name|ITtlTimeProvider
name|ttlTp
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|metas
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// Happens when someone calls put() with an empty list.
name|LOG
operator|.
name|debug
argument_list|(
literal|"Ignoring empty list of entries to put"
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// always create or retrieve an ancestor state instance, so it can
comment|// always be used for synchronization.
specifier|final
name|AncestorState
name|ancestorState
init|=
name|extractOrCreate
argument_list|(
name|operationState
argument_list|,
name|BulkOperationState
operator|.
name|OperationType
operator|.
name|Put
argument_list|)
decl_stmt|;
name|Item
index|[]
name|items
decl_stmt|;
synchronized|synchronized
init|(
name|ancestorState
init|)
block|{
name|items
operator|=
name|pathMetadataToItem
argument_list|(
name|completeAncestry
argument_list|(
name|metas
argument_list|,
name|ancestorState
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Saving batch of {} items to table {}, region {}"
argument_list|,
name|items
operator|.
name|length
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|)
expr_stmt|;
name|processBatchWriteRequest
argument_list|(
name|ancestorState
argument_list|,
literal|null
argument_list|,
name|items
argument_list|)
expr_stmt|;
block|}
comment|/**    * Get full path of ancestors that are nonexistent in table.    *    * This queries DDB when looking for parents which are not in    * any supplied ongoing operation state.    * Updates the operation state with found entries to reduce further checks.    *    * @param meta metadata to put    * @param operationState ongoing bulk state    * @return a possibly empty list of entries to put.    * @throws IOException failure    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"SynchronizationOnLocalVariableOrMethodParameter"
argument_list|)
annotation|@
name|VisibleForTesting
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|fullPathsToPut (DDBPathMetadata meta, @Nullable BulkOperationState operationState)
name|List
argument_list|<
name|DDBPathMetadata
argument_list|>
name|fullPathsToPut
parameter_list|(
name|DDBPathMetadata
name|meta
parameter_list|,
annotation|@
name|Nullable
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
name|checkPathMetadata
argument_list|(
name|meta
argument_list|)
expr_stmt|;
specifier|final
name|List
argument_list|<
name|DDBPathMetadata
argument_list|>
name|metasToPut
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
comment|// root path is not persisted
if|if
condition|(
operator|!
name|meta
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
operator|.
name|isRoot
argument_list|()
condition|)
block|{
name|metasToPut
operator|.
name|add
argument_list|(
name|meta
argument_list|)
expr_stmt|;
block|}
comment|// put all its ancestors if not present; as an optimization we return at its
comment|// first existent ancestor
specifier|final
name|AncestorState
name|ancestorState
init|=
name|extractOrCreate
argument_list|(
name|operationState
argument_list|,
name|BulkOperationState
operator|.
name|OperationType
operator|.
name|Put
argument_list|)
decl_stmt|;
name|Path
name|path
init|=
name|meta
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
operator|.
name|getParent
argument_list|()
decl_stmt|;
while|while
condition|(
name|path
operator|!=
literal|null
operator|&&
operator|!
name|path
operator|.
name|isRoot
argument_list|()
condition|)
block|{
synchronized|synchronized
init|(
name|ancestorState
init|)
block|{
if|if
condition|(
name|ancestorState
operator|.
name|findEntry
argument_list|(
name|path
argument_list|,
literal|true
argument_list|)
condition|)
block|{
break|break;
block|}
block|}
specifier|final
name|Item
name|item
init|=
name|getConsistentItem
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|itemExists
argument_list|(
name|item
argument_list|)
condition|)
block|{
specifier|final
name|S3AFileStatus
name|status
init|=
name|makeDirStatus
argument_list|(
name|path
argument_list|,
name|username
argument_list|)
decl_stmt|;
name|metasToPut
operator|.
name|add
argument_list|(
operator|new
name|DDBPathMetadata
argument_list|(
name|status
argument_list|,
name|Tristate
operator|.
name|FALSE
argument_list|,
literal|false
argument_list|,
name|meta
operator|.
name|isAuthoritativeDir
argument_list|()
argument_list|,
name|meta
operator|.
name|getLastUpdated
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|path
operator|=
name|path
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
else|else
block|{
comment|// found the entry in the table, so add it to the ancestor state
synchronized|synchronized
init|(
name|ancestorState
init|)
block|{
name|ancestorState
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|itemToPathMetadata
argument_list|(
name|item
argument_list|,
name|username
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// then break out of the loop.
break|break;
block|}
block|}
return|return
name|metasToPut
return|;
block|}
comment|/**    * Does an item represent an object which exists?    * @param item item retrieved in a query.    * @return true iff the item isn't null and, if there is an is_deleted    * column, that its value is false.    */
DECL|method|itemExists (Item item)
specifier|private
specifier|static
name|boolean
name|itemExists
parameter_list|(
name|Item
name|item
parameter_list|)
block|{
if|if
condition|(
name|item
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
if|if
condition|(
name|item
operator|.
name|hasAttribute
argument_list|(
name|IS_DELETED
argument_list|)
operator|&&
name|item
operator|.
name|getBoolean
argument_list|(
name|IS_DELETED
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
return|return
literal|true
return|;
block|}
comment|/** Create a directory FileStatus using 0 for the lastUpdated time. */
DECL|method|makeDirStatus (Path f, String owner)
specifier|static
name|S3AFileStatus
name|makeDirStatus
parameter_list|(
name|Path
name|f
parameter_list|,
name|String
name|owner
parameter_list|)
block|{
return|return
operator|new
name|S3AFileStatus
argument_list|(
name|Tristate
operator|.
name|UNKNOWN
argument_list|,
name|f
argument_list|,
name|owner
argument_list|)
return|;
block|}
comment|/**    * {@inheritDoc}.    * There is retry around building the list of paths to update, but    * the call to    * {@link #processBatchWriteRequest(DynamoDBMetadataStore.AncestorState, PrimaryKey[], Item[])}    * is only tried once.    * @param meta Directory listing metadata.    * @param operationState operational state for a bulk update    * @throws IOException IO problem    */
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|put ( final DirListingMetadata meta, @Nullable final BulkOperationState operationState)
specifier|public
name|void
name|put
parameter_list|(
specifier|final
name|DirListingMetadata
name|meta
parameter_list|,
annotation|@
name|Nullable
specifier|final
name|BulkOperationState
name|operationState
parameter_list|)
throws|throws
name|IOException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Saving {} dir meta for {} to table {} in region {}: {}"
argument_list|,
name|tableName
argument_list|,
name|meta
operator|.
name|isAuthoritative
argument_list|()
condition|?
literal|"auth"
else|:
literal|"nonauth"
argument_list|,
name|meta
operator|.
name|getPath
argument_list|()
argument_list|,
name|tableName
argument_list|,
name|region
argument_list|,
name|meta
argument_list|)
expr_stmt|;
comment|// directory path
name|Path
name|path
init|=
name|meta
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|DDBPathMetadata
name|ddbPathMeta
init|=
operator|new
name|DDBPathMetadata
argument_list|(
name|makeDirStatus
argument_list|(
name|path
argument_list|,
name|username
argument_list|)
argument_list|,
name|meta
operator|.
name|isEmpty
argument_list|()
argument_list|,
literal|false
argument_list|,
name|meta
operator|.
name|isAuthoritative
argument_list|()
argument_list|,
name|meta
operator|.
name|getLastUpdated
argument_list|()
argument_list|)
decl_stmt|;
comment|// put all its ancestors if not present
specifier|final
name|AncestorState
name|ancestorState
init|=
name|extractOrCreate
argument_list|(
name|operationState
argument_list|,
name|BulkOperationState
operator|.
name|OperationType
operator|.
name|Put
argument_list|)
decl_stmt|;
comment|// First add any missing ancestors...
specifier|final
name|List
argument_list|<
name|DDBPathMetadata
argument_list|>
name|metasToPut
init|=
name|fullPathsToPut
argument_list|(
name|ddbPathMeta
argument_list|,
name|ancestorState
argument_list|)
decl_stmt|;
comment|// next add all children of the directory
name|metasToPut
operator|.
name|addAll
argument_list|(
name|pathMetaToDDBPathMeta
argument_list|(
name|meta
operator|.
name|getListing
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// sort so highest-level entries are written to the store first.
comment|// if a sequence fails, no orphan entries will have been written.
name|metasToPut
operator|.
name|sort
argument_list|(
name|PathOrderComparators
operator|.
name|TOPMOST_PM_FIRST
argument_list|)
expr_stmt|;
name|processBatchWriteRequest
argument_list|(
name|ancestorState
argument_list|,
literal|null
argument_list|,
name|pathMetadataToItem
argument_list|(
name|metasToPut
argument_list|)
argument_list|)
expr_stmt|;
comment|// and add the ancestors
synchronized|synchronized
init|(
name|ancestorState
init|)
block|{
name|metasToPut
operator|.
name|forEach
argument_list|(
name|ancestorState
operator|::
name|put
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
DECL|method|close ()
specifier|public
specifier|synchronized
name|void
name|close
parameter_list|()
block|{
if|if
condition|(
name|instrumentation
operator|!=
literal|null
condition|)
block|{
name|instrumentation
operator|.
name|storeClosed
argument_list|()
expr_stmt|;
block|}
try|try
block|{
if|if
condition|(
name|dynamoDB
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Shutting down {}"
argument_list|,
name|this
argument_list|)
expr_stmt|;
name|dynamoDB
operator|.
name|shutdown
argument_list|()
expr_stmt|;
name|dynamoDB
operator|=
literal|null
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|closeAutocloseables
argument_list|(
name|LOG
argument_list|,
name|credentials
argument_list|)
expr_stmt|;
name|credentials
operator|=
literal|null
expr_stmt|;
block|}
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|destroy ()
specifier|public
name|void
name|destroy
parameter_list|()
throws|throws
name|IOException
block|{
name|tableHandler
operator|.
name|destroy
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|expiredFiles (PruneMode pruneMode, long cutoff, String keyPrefix)
specifier|private
name|ItemCollection
argument_list|<
name|ScanOutcome
argument_list|>
name|expiredFiles
parameter_list|(
name|PruneMode
name|pruneMode
parameter_list|,
name|long
name|cutoff
parameter_list|,
name|String
name|keyPrefix
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|filterExpression
decl_stmt|;
name|String
name|projectionExpression
decl_stmt|;
name|ValueMap
name|map
decl_stmt|;
switch|switch
condition|(
name|pruneMode
condition|)
block|{
case|case
name|ALL_BY_MODTIME
case|:
name|filterExpression
operator|=
literal|"mod_time< :mod_time and begins_with(parent, :parent)"
expr_stmt|;
name|projectionExpression
operator|=
literal|"parent,child"
expr_stmt|;
name|map
operator|=
operator|new
name|ValueMap
argument_list|()
operator|.
name|withLong
argument_list|(
literal|":mod_time"
argument_list|,
name|cutoff
argument_list|)
operator|.
name|withString
argument_list|(
literal|":parent"
argument_list|,
name|keyPrefix
argument_list|)
expr_stmt|;
break|break;
case|case
name|TOMBSTONES_BY_LASTUPDATED
case|:
name|filterExpression
operator|=
literal|"last_updated< :last_updated and begins_with(parent, :parent) "
operator|+
literal|"and is_deleted = :is_deleted"
expr_stmt|;
name|projectionExpression
operator|=
literal|"parent,child"
expr_stmt|;
name|map
operator|=
operator|new
name|ValueMap
argument_list|()
operator|.
name|withLong
argument_list|(
literal|":last_updated"
argument_list|,
name|cutoff
argument_list|)
operator|.
name|withString
argument_list|(
literal|":parent"
argument_list|,
name|keyPrefix
argument_list|)
operator|.
name|withBoolean
argument_list|(
literal|":is_deleted"
argument_list|,
literal|true
argument_list|)
expr_stmt|;
break|break;
default|default:
throw|throw
operator|new
name|UnsupportedOperationException
argument_list|(
literal|"Unsupported prune mode: "
operator|+
name|pruneMode
argument_list|)
throw|;
block|}
return|return
name|readOp
operator|.
name|retry
argument_list|(
literal|"scan"
argument_list|,
name|keyPrefix
argument_list|,
literal|true
argument_list|,
parameter_list|()
lambda|->
name|table
operator|.
name|scan
argument_list|(
name|filterExpression
argument_list|,
name|projectionExpression
argument_list|,
literal|null
argument_list|,
name|map
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|prune (PruneMode pruneMode, long cutoff)
specifier|public
name|void
name|prune
parameter_list|(
name|PruneMode
name|pruneMode
parameter_list|,
name|long
name|cutoff
parameter_list|)
throws|throws
name|IOException
block|{
name|prune
argument_list|(
name|pruneMode
argument_list|,
name|cutoff
argument_list|,
literal|"/"
argument_list|)
expr_stmt|;
block|}
comment|/**    * Prune files, in batches. There's a sleep between each batch.    *    * @param pruneMode The mode of operation for the prune For details see    *                  {@link MetadataStore#prune(PruneMode, long)}    * @param cutoff Oldest modification time to allow    * @param keyPrefix The prefix for the keys that should be removed    * @throws IOException Any IO/DDB failure.    * @throws InterruptedIOException if the prune was interrupted    */
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|RetryTranslated
DECL|method|prune (PruneMode pruneMode, long cutoff, String keyPrefix)
specifier|public
name|void
name|prune
parameter_list|(
name|PruneMode
name|pruneMode
parameter_list|,
name|long
name|cutoff
parameter_list|,
name|String
name|keyPrefix
parameter_list|)
throws|throws
name|IOException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Prune {} under {} with age {}"
argument_list|,
name|pruneMode
operator|==
name|PruneMode
operator|.
name|ALL_BY_MODTIME
condition|?
literal|"files and tombstones"
else|:
literal|"tombstones"
argument_list|,
name|keyPrefix
argument_list|,
name|cutoff
argument_list|)
expr_stmt|;
specifier|final
name|ItemCollection
argument_list|<
name|ScanOutcome
argument_list|>
name|items
init|=
name|expiredFiles
argument_list|(
name|pruneMode
argument_list|,
name|cutoff
argument_list|,
name|keyPrefix
argument_list|)
decl_stmt|;
name|innerPrune
argument_list|(
name|keyPrefix
argument_list|,
name|items
argument_list|)
expr_stmt|;
block|}
DECL|method|innerPrune (String keyPrefix, ItemCollection<ScanOutcome> items)
specifier|private
name|void
name|innerPrune
parameter_list|(
name|String
name|keyPrefix
parameter_list|,
name|ItemCollection
argument_list|<
name|ScanOutcome
argument_list|>
name|items
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|itemCount
init|=
literal|0
decl_stmt|;
try|try
init|(
name|AncestorState
name|state
init|=
name|initiateBulkWrite
argument_list|(
name|BulkOperationState
operator|.
name|OperationType
operator|.
name|Prune
argument_list|,
literal|null
argument_list|)
init|;
name|DurationInfo
name|ignored
operator|=
operator|new
name|DurationInfo
argument_list|(
name|LOG
argument_list|,
literal|"Pruning DynamoDB Store"
argument_list|)
init|)
block|{
name|ArrayList
argument_list|<
name|Path
argument_list|>
name|deletionBatch
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT
argument_list|)
decl_stmt|;
name|long
name|delay
init|=
name|conf
operator|.
name|getTimeDuration
argument_list|(
name|S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY
argument_list|,
name|S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT
argument_list|,
name|TimeUnit
operator|.
name|MILLISECONDS
argument_list|)
decl_stmt|;
name|Set
argument_list|<
name|Path
argument_list|>
name|parentPathSet
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|Path
argument_list|>
name|clearedParentPathSet
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|Item
name|item
range|:
name|items
control|)
block|{
name|DDBPathMetadata
name|md
init|=
name|PathMetadataDynamoDBTranslation
operator|.
name|itemToPathMetadata
argument_list|(
name|item
argument_list|,
name|username
argument_list|)
decl_stmt|;
name|Path
name|path
init|=
name|md
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|boolean
name|tombstone
init|=
name|md
operator|.
name|isDeleted
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Prune entry {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|deletionBatch
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
comment|// add parent path of item so it can be marked as non-auth.
comment|// this is only done if
comment|// * it has not already been processed
comment|// * the entry pruned is not a tombstone (no need to update)
comment|// * the file is not in the root dir
name|Path
name|parentPath
init|=
name|path
operator|.
name|getParent
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|tombstone
operator|&&
name|parentPath
operator|!=
literal|null
operator|&&
operator|!
name|clearedParentPathSet
operator|.
name|contains
argument_list|(
name|parentPath
argument_list|)
condition|)
block|{
name|parentPathSet
operator|.
name|add
argument_list|(
name|parentPath
argument_list|)
expr_stmt|;
block|}
name|itemCount
operator|++
expr_stmt|;
if|if
condition|(
name|deletionBatch
operator|.
name|size
argument_list|()
operator|==
name|S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT
condition|)
block|{
comment|// lowest path entries get deleted first.
name|deletionBatch
operator|.
name|sort
argument_list|(
name|PathOrderComparators
operator|.
name|TOPMOST_PATH_LAST
argument_list|)
expr_stmt|;
name|processBatchWriteRequest
argument_list|(
name|state
argument_list|,
name|pathToKey
argument_list|(
name|deletionBatch
argument_list|)
argument_list|,
literal|null
argument_list|)
expr_stmt|;
comment|// set authoritative false for each pruned dir listing
name|removeAuthoritativeDirFlag
argument_list|(
name|parentPathSet
argument_list|,
name|state
argument_list|)
expr_stmt|;
comment|// already cleared parent paths.
name|clearedParentPathSet
operator|.
name|addAll
argument_list|(
name|parentPathSet
argument_list|)
expr_stmt|;
name|parentPathSet
operator|.
name|clear
argument_list|()
expr_stmt|;
name|deletionBatch
operator|.
name|clear
argument_list|()
expr_stmt|;
if|if
condition|(
name|delay
operator|>
literal|0
condition|)
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|delay
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// final batch of deletes
if|if
condition|(
operator|!
name|deletionBatch
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|processBatchWriteRequest
argument_list|(
name|state
argument_list|,
name|pathToKey
argument_list|(
name|deletionBatch
argument_list|)
argument_list|,
literal|null
argument_list|)
expr_stmt|;
comment|// set authoritative false for each pruned dir listing
name|removeAuthoritativeDirFlag
argument_list|(
name|parentPathSet
argument_list|,
name|state
argument_list|)
expr_stmt|;
name|parentPathSet
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|interrupt
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|InterruptedIOException
argument_list|(
literal|"Pruning was interrupted"
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|AmazonDynamoDBException
name|e
parameter_list|)
block|{
throw|throw
name|translateDynamoDBException
argument_list|(
name|keyPrefix
argument_list|,
literal|"Prune of "
operator|+
name|keyPrefix
operator|+
literal|" failed"
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Finished pruning {} items in batches of {}"
argument_list|,
name|itemCount
argument_list|,
name|S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT
argument_list|)
expr_stmt|;
block|}
comment|/**    * Remove the Authoritative Directory Marker from a set of paths, if    * those paths are in the store.    *<p>    * This operation is<i>only</i>for pruning; it does not raise an error    * if, during the prune phase, the table appears inconsistent.    * This is not unusual as it can happen in a number of ways    *<ol>    *<li>The state of the table changes during a slow prune operation which    *   deliberately inserts pauses to avoid overloading prepaid IO capacity.    *</li>    *<li>Tombstone markers have been left in the table after many other    *   operations have taken place, including deleting/replacing    *   parents.</li>    *</ol>    *<p>    *    * If an exception is raised in the get/update process, then the exception    * is caught and only rethrown after all the other paths are processed.    * This is to ensure a best-effort attempt to update the store.    * @param pathSet set of paths.    * @param state ongoing operation state.    * @throws IOException only after a best effort is made to update the store.    */
DECL|method|removeAuthoritativeDirFlag ( final Set<Path> pathSet, final AncestorState state)
specifier|private
name|void
name|removeAuthoritativeDirFlag
parameter_list|(
specifier|final
name|Set
argument_list|<
name|Path
argument_list|>
name|pathSet
parameter_list|,
specifier|final
name|AncestorState
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|AtomicReference
argument_list|<
name|IOException
argument_list|>
name|rIOException
init|=
operator|new
name|AtomicReference
argument_list|<>
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|DDBPathMetadata
argument_list|>
name|metas
init|=
name|pathSet
operator|.
name|stream
argument_list|()
operator|.
name|map
argument_list|(
name|path
lambda|->
block|{
try|try
block|{
if|if
condition|(
name|state
operator|!=
literal|null
operator|&&
name|state
operator|.
name|get
argument_list|(
name|path
argument_list|)
operator|!=
literal|null
condition|)
block|{
comment|// there's already an entry for this path
name|LOG
operator|.
name|debug
argument_list|(
literal|"Ignoring update of entry already in the state map"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|DDBPathMetadata
name|ddbPathMetadata
init|=
name|get
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|ddbPathMetadata
operator|==
literal|null
condition|)
block|{
comment|// there is no entry.
name|LOG
operator|.
name|debug
argument_list|(
literal|"No parent {}; skipping"
argument_list|,
name|path
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
if|if
condition|(
name|ddbPathMetadata
operator|.
name|isDeleted
argument_list|()
condition|)
block|{
comment|// the parent itself is deleted
name|LOG
operator|.
name|debug
argument_list|(
literal|"Parent has been deleted {}; skipping"
argument_list|,
name|path
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
if|if
condition|(
operator|!
name|ddbPathMetadata
operator|.
name|getFileStatus
argument_list|()
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
comment|// the parent itself is deleted
name|LOG
operator|.
name|debug
argument_list|(
literal|"Parent is not a directory {}; skipping"
argument_list|,
name|path
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Setting isAuthoritativeDir==false on {}"
argument_list|,
name|ddbPathMetadata
argument_list|)
expr_stmt|;
name|ddbPathMetadata
operator|.
name|setAuthoritativeDir
argument_list|(
literal|false
argument_list|)
expr_stmt|;
return|return
name|ddbPathMetadata
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
name|String
operator|.
name|format
argument_list|(
literal|"IOException while getting PathMetadata "
operator|+
literal|"on path: %s."
argument_list|,
name|path
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|rIOException
operator|.
name|set
argument_list|(
name|e
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
block|}
argument_list|)
operator|.
name|filter
argument_list|(
name|Objects
operator|::
name|nonNull
argument_list|)
operator|.
name|collect
argument_list|(
name|Collectors
operator|.
name|toSet
argument_list|()
argument_list|)
decl_stmt|;
try|try
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"innerPut on metas: {}"
argument_list|,
name|metas
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|metas
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|innerPut
argument_list|(
name|metas
argument_list|,
name|state
argument_list|,
name|ttlTimeProvider
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
name|String
operator|.
name|format
argument_list|(
literal|"IOException while setting false "
operator|+
literal|"authoritative directory flag on: %s."
argument_list|,
name|metas
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|rIOException
operator|.
name|set
argument_list|(
name|e
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|rIOException
operator|.
name|get
argument_list|()
operator|!=
literal|null
condition|)
block|{
throw|throw
name|rIOException
operator|.
name|get
argument_list|()
throw|;
block|}
block|}
annotation|@
name|VisibleForTesting
DECL|method|getAmazonDynamoDB ()
specifier|public
name|AmazonDynamoDB
name|getAmazonDynamoDB
parameter_list|()
block|{
return|return
name|amazonDynamoDB
return|;
block|}
annotation|@
name|Override
DECL|method|toString ()
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|getClass
argument_list|()
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|'{'
operator|+
literal|"region="
operator|+
name|region
operator|+
literal|", tableName="
operator|+
name|tableName
operator|+
literal|", tableArn="
operator|+
name|tableHandler
operator|.
name|getTableArn
argument_list|()
operator|+
literal|'}'
return|;
block|}
comment|/**    * The administrative policy includes all DDB table operations;    * application access is restricted to those operations S3Guard operations    * require when working with data in a guarded bucket.    * @param access access level desired.    * @return a possibly empty list of statements.    */
annotation|@
name|Override
DECL|method|listAWSPolicyRules ( final Set<AccessLevel> access)
specifier|public
name|List
argument_list|<
name|RoleModel
operator|.
name|Statement
argument_list|>
name|listAWSPolicyRules
parameter_list|(
specifier|final
name|Set
argument_list|<
name|AccessLevel
argument_list|>
name|access
parameter_list|)
block|{
name|Preconditions
operator|.
name|checkState
argument_list|(
name|tableHandler
operator|.
name|getTableArn
argument_list|()
operator|!=
literal|null
argument_list|,
literal|"TableARN not known"
argument_list|)
expr_stmt|;
if|if
condition|(
name|access
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
name|Collections
operator|.
name|emptyList
argument_list|()
return|;
block|}
name|RoleModel
operator|.
name|Statement
name|stat
decl_stmt|;
if|if
condition|(
name|access
operator|.
name|contains
argument_list|(
name|AccessLevel
operator|.
name|ADMIN
argument_list|)
condition|)
block|{
name|stat
operator|=
name|allowAllDynamoDBOperations
argument_list|(
name|tableHandler
operator|.
name|getTableArn
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|stat
operator|=
name|allowS3GuardClientOperations
argument_list|(
name|tableHandler
operator|.
name|getTableArn
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|Lists
operator|.
name|newArrayList
argument_list|(
name|stat
argument_list|)
return|;
block|}
comment|/**    * PUT a single item to the table.    * @param item item to put    * @return the outcome.    */
annotation|@
name|Retries
operator|.
name|OnceRaw
DECL|method|putItem (Item item)
specifier|private
name|PutItemOutcome
name|putItem
parameter_list|(
name|Item
name|item
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Putting item {}"
argument_list|,
name|item
argument_list|)
expr_stmt|;
return|return
name|table
operator|.
name|putItem
argument_list|(
name|item
argument_list|)
return|;
block|}
annotation|@
name|VisibleForTesting
DECL|method|getTable ()
name|Table
name|getTable
parameter_list|()
block|{
return|return
name|table
return|;
block|}
DECL|method|getRegion ()
name|String
name|getRegion
parameter_list|()
block|{
return|return
name|region
return|;
block|}
annotation|@
name|VisibleForTesting
DECL|method|getTableName ()
specifier|public
name|String
name|getTableName
parameter_list|()
block|{
return|return
name|tableName
return|;
block|}
annotation|@
name|VisibleForTesting
DECL|method|getDynamoDB ()
name|DynamoDB
name|getDynamoDB
parameter_list|()
block|{
return|return
name|dynamoDB
return|;
block|}
comment|/**    * Validates a path object; it must be absolute, have an s3a:/// scheme    * and contain a host (bucket) component.    * @param path path to check    * @return the path passed in    */
DECL|method|checkPath (Path path)
specifier|private
name|Path
name|checkPath
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|path
operator|.
name|isAbsolute
argument_list|()
argument_list|,
literal|"Path %s is not absolute"
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|URI
name|uri
init|=
name|path
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|uri
operator|.
name|getScheme
argument_list|()
argument_list|,
literal|"Path %s missing scheme"
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|uri
operator|.
name|getScheme
argument_list|()
operator|.
name|equals
argument_list|(
name|Constants
operator|.
name|FS_S3A
argument_list|)
argument_list|,
literal|"Path %s scheme must be %s"
argument_list|,
name|path
argument_list|,
name|Constants
operator|.
name|FS_S3A
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|uri
operator|.
name|getHost
argument_list|()
argument_list|)
argument_list|,
literal|"Path %s"
operator|+
literal|" is missing bucket."
argument_list|,
name|path
argument_list|)
expr_stmt|;
return|return
name|path
return|;
block|}
comment|/**    * Validates a path meta-data object.    */
DECL|method|checkPathMetadata (PathMetadata meta)
specifier|private
specifier|static
name|void
name|checkPathMetadata
parameter_list|(
name|PathMetadata
name|meta
parameter_list|)
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|meta
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|meta
operator|.
name|getFileStatus
argument_list|()
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|meta
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|OnceRaw
DECL|method|getDiagnostics ()
specifier|public
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|getDiagnostics
parameter_list|()
throws|throws
name|IOException
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|map
init|=
operator|new
name|TreeMap
argument_list|<>
argument_list|()
decl_stmt|;
if|if
condition|(
name|table
operator|!=
literal|null
condition|)
block|{
name|TableDescription
name|desc
init|=
name|getTableDescription
argument_list|(
literal|true
argument_list|)
decl_stmt|;
name|map
operator|.
name|put
argument_list|(
literal|"name"
argument_list|,
name|desc
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
name|STATUS
argument_list|,
name|desc
operator|.
name|getTableStatus
argument_list|()
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
literal|"ARN"
argument_list|,
name|desc
operator|.
name|getTableArn
argument_list|()
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
literal|"size"
argument_list|,
name|desc
operator|.
name|getTableSizeBytes
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
name|TABLE
argument_list|,
name|desc
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|ProvisionedThroughputDescription
name|throughput
init|=
name|desc
operator|.
name|getProvisionedThroughput
argument_list|()
decl_stmt|;
name|map
operator|.
name|put
argument_list|(
name|READ_CAPACITY
argument_list|,
name|throughput
operator|.
name|getReadCapacityUnits
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
name|WRITE_CAPACITY
argument_list|,
name|throughput
operator|.
name|getWriteCapacityUnits
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
name|BILLING_MODE
argument_list|,
name|throughput
operator|.
name|getWriteCapacityUnits
argument_list|()
operator|==
literal|0
condition|?
name|BILLING_MODE_PER_REQUEST
else|:
name|BILLING_MODE_PROVISIONED
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
name|TABLE
argument_list|,
name|desc
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
name|MetadataStoreCapabilities
operator|.
name|PERSISTS_AUTHORITATIVE_BIT
argument_list|,
name|Boolean
operator|.
name|toString
argument_list|(
literal|true
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|map
operator|.
name|put
argument_list|(
literal|"name"
argument_list|,
literal|"DynamoDB Metadata Store"
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
name|TABLE
argument_list|,
literal|"none"
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
name|STATUS
argument_list|,
literal|"undefined"
argument_list|)
expr_stmt|;
block|}
name|map
operator|.
name|put
argument_list|(
literal|"description"
argument_list|,
name|DESCRIPTION
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
literal|"region"
argument_list|,
name|region
argument_list|)
expr_stmt|;
if|if
condition|(
name|batchWriteRetryPolicy
operator|!=
literal|null
condition|)
block|{
name|map
operator|.
name|put
argument_list|(
literal|"retryPolicy"
argument_list|,
name|batchWriteRetryPolicy
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|map
return|;
block|}
annotation|@
name|Retries
operator|.
name|OnceRaw
DECL|method|getTableDescription (boolean forceUpdate)
specifier|private
name|TableDescription
name|getTableDescription
parameter_list|(
name|boolean
name|forceUpdate
parameter_list|)
block|{
name|TableDescription
name|desc
init|=
name|table
operator|.
name|getDescription
argument_list|()
decl_stmt|;
if|if
condition|(
name|desc
operator|==
literal|null
operator|||
name|forceUpdate
condition|)
block|{
name|desc
operator|=
name|table
operator|.
name|describe
argument_list|()
expr_stmt|;
block|}
return|return
name|desc
return|;
block|}
annotation|@
name|Override
annotation|@
name|Retries
operator|.
name|OnceRaw
DECL|method|updateParameters (Map<String, String> parameters)
specifier|public
name|void
name|updateParameters
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|parameters
parameter_list|)
throws|throws
name|IOException
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|table
argument_list|,
literal|"Not initialized"
argument_list|)
expr_stmt|;
name|TableDescription
name|desc
init|=
name|getTableDescription
argument_list|(
literal|true
argument_list|)
decl_stmt|;
name|ProvisionedThroughputDescription
name|current
init|=
name|desc
operator|.
name|getProvisionedThroughput
argument_list|()
decl_stmt|;
name|long
name|currentRead
init|=
name|current
operator|.
name|getReadCapacityUnits
argument_list|()
decl_stmt|;
name|long
name|newRead
init|=
name|getLongParam
argument_list|(
name|parameters
argument_list|,
name|S3GUARD_DDB_TABLE_CAPACITY_READ_KEY
argument_list|,
name|currentRead
argument_list|)
decl_stmt|;
name|long
name|currentWrite
init|=
name|current
operator|.
name|getWriteCapacityUnits
argument_list|()
decl_stmt|;
name|long
name|newWrite
init|=
name|getLongParam
argument_list|(
name|parameters
argument_list|,
name|S3GUARD_DDB_TABLE_CAPACITY_WRITE_KEY
argument_list|,
name|currentWrite
argument_list|)
decl_stmt|;
if|if
condition|(
name|currentRead
operator|==
literal|0
operator|||
name|currentWrite
operator|==
literal|0
condition|)
block|{
comment|// table is pay on demand
throw|throw
operator|new
name|IOException
argument_list|(
name|E_ON_DEMAND_NO_SET_CAPACITY
argument_list|)
throw|;
block|}
if|if
condition|(
name|newRead
operator|!=
name|currentRead
operator|||
name|newWrite
operator|!=
name|currentWrite
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Current table capacity is read: {}, write: {}"
argument_list|,
name|currentRead
argument_list|,
name|currentWrite
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Changing capacity of table to read: {}, write: {}"
argument_list|,
name|newRead
argument_list|,
name|newWrite
argument_list|)
expr_stmt|;
name|tableHandler
operator|.
name|provisionTableBlocking
argument_list|(
name|newRead
argument_list|,
name|newWrite
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Table capacity unchanged at read: {}, write: {}"
argument_list|,
name|newRead
argument_list|,
name|newWrite
argument_list|)
expr_stmt|;
block|}
block|}
DECL|method|getLongParam (Map<String, String> parameters, String key, long defVal)
specifier|private
name|long
name|getLongParam
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|parameters
parameter_list|,
name|String
name|key
parameter_list|,
name|long
name|defVal
parameter_list|)
block|{
name|String
name|k
init|=
name|parameters
operator|.
name|get
argument_list|(
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|k
operator|!=
literal|null
condition|)
block|{
return|return
name|Long
operator|.
name|parseLong
argument_list|(
name|k
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|defVal
return|;
block|}
block|}
comment|/**    * Callback on a read operation retried.    * @param text text of the operation    * @param ex exception    * @param attempts number of attempts    * @param idempotent is the method idempotent (this is assumed to be true)    */
DECL|method|readRetryEvent ( String text, IOException ex, int attempts, boolean idempotent)
name|void
name|readRetryEvent
parameter_list|(
name|String
name|text
parameter_list|,
name|IOException
name|ex
parameter_list|,
name|int
name|attempts
parameter_list|,
name|boolean
name|idempotent
parameter_list|)
block|{
name|readThrottleEvents
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
name|retryEvent
argument_list|(
name|text
argument_list|,
name|ex
argument_list|,
name|attempts
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
comment|/**    * Callback  on a write operation retried.    * @param text text of the operation    * @param ex exception    * @param attempts number of attempts    * @param idempotent is the method idempotent (this is assumed to be true)    */
DECL|method|writeRetryEvent ( String text, IOException ex, int attempts, boolean idempotent)
name|void
name|writeRetryEvent
parameter_list|(
name|String
name|text
parameter_list|,
name|IOException
name|ex
parameter_list|,
name|int
name|attempts
parameter_list|,
name|boolean
name|idempotent
parameter_list|)
block|{
name|writeThrottleEvents
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
name|retryEvent
argument_list|(
name|text
argument_list|,
name|ex
argument_list|,
name|attempts
argument_list|,
name|idempotent
argument_list|)
expr_stmt|;
block|}
comment|/**    * Callback from {@link Invoker} when an operation is retried.    * @param text text of the operation    * @param ex exception    * @param attempts number of attempts    * @param idempotent is the method idempotent    */
DECL|method|retryEvent ( String text, IOException ex, int attempts, boolean idempotent)
name|void
name|retryEvent
parameter_list|(
name|String
name|text
parameter_list|,
name|IOException
name|ex
parameter_list|,
name|int
name|attempts
parameter_list|,
name|boolean
name|idempotent
parameter_list|)
block|{
if|if
condition|(
name|S3AUtils
operator|.
name|isThrottleException
argument_list|(
name|ex
argument_list|)
condition|)
block|{
comment|// throttled
if|if
condition|(
name|instrumentation
operator|!=
literal|null
condition|)
block|{
name|instrumentation
operator|.
name|throttled
argument_list|()
expr_stmt|;
block|}
name|int
name|eventCount
init|=
name|throttleEventCount
operator|.
name|addAndGet
argument_list|(
literal|1
argument_list|)
decl_stmt|;
if|if
condition|(
name|attempts
operator|==
literal|1
operator|&&
name|eventCount
operator|<
name|THROTTLE_EVENT_LOG_LIMIT
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"DynamoDB IO limits reached in {};"
operator|+
literal|" consider increasing capacity: {}"
argument_list|,
name|text
argument_list|,
name|ex
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Throttled"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// user has been warned already, log at debug only.
name|LOG
operator|.
name|debug
argument_list|(
literal|"DynamoDB IO limits reached in {};"
operator|+
literal|" consider increasing capacity: {}"
argument_list|,
name|text
argument_list|,
name|ex
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|attempts
operator|==
literal|1
condition|)
block|{
comment|// not throttled. Log on the first attempt only
name|LOG
operator|.
name|info
argument_list|(
literal|"Retrying {}: {}"
argument_list|,
name|text
argument_list|,
name|ex
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Retrying {}"
argument_list|,
name|text
argument_list|,
name|ex
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|instrumentation
operator|!=
literal|null
condition|)
block|{
comment|// note a retry
name|instrumentation
operator|.
name|retrying
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|owner
operator|!=
literal|null
condition|)
block|{
name|owner
operator|.
name|metastoreOperationRetried
argument_list|(
name|ex
argument_list|,
name|attempts
argument_list|,
name|idempotent
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Get the count of read throttle events.    * @return the current count of read throttle events.    */
annotation|@
name|VisibleForTesting
DECL|method|getReadThrottleEventCount ()
specifier|public
name|long
name|getReadThrottleEventCount
parameter_list|()
block|{
return|return
name|readThrottleEvents
operator|.
name|get
argument_list|()
return|;
block|}
comment|/**    * Get the count of write throttle events.    * @return the current count of write throttle events.    */
annotation|@
name|VisibleForTesting
DECL|method|getWriteThrottleEventCount ()
specifier|public
name|long
name|getWriteThrottleEventCount
parameter_list|()
block|{
return|return
name|writeThrottleEvents
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|VisibleForTesting
DECL|method|getBatchWriteCapacityExceededCount ()
specifier|public
name|long
name|getBatchWriteCapacityExceededCount
parameter_list|()
block|{
return|return
name|batchWriteCapacityExceededEvents
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|VisibleForTesting
DECL|method|getInvoker ()
specifier|public
name|Invoker
name|getInvoker
parameter_list|()
block|{
return|return
name|invoker
return|;
block|}
comment|/**    * Record the number of records written.    * @param count count of records.    */
DECL|method|recordsWritten (final int count)
specifier|private
name|void
name|recordsWritten
parameter_list|(
specifier|final
name|int
name|count
parameter_list|)
block|{
if|if
condition|(
name|instrumentation
operator|!=
literal|null
condition|)
block|{
name|instrumentation
operator|.
name|recordsWritten
argument_list|(
name|count
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Record the number of records read.    * @param count count of records.    */
DECL|method|recordsRead (final int count)
specifier|private
name|void
name|recordsRead
parameter_list|(
specifier|final
name|int
name|count
parameter_list|)
block|{
if|if
condition|(
name|instrumentation
operator|!=
literal|null
condition|)
block|{
name|instrumentation
operator|.
name|recordsRead
argument_list|(
name|count
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Record the number of records deleted.    * @param count count of records.    */
DECL|method|recordsDeleted (final int count)
specifier|private
name|void
name|recordsDeleted
parameter_list|(
specifier|final
name|int
name|count
parameter_list|)
block|{
if|if
condition|(
name|instrumentation
operator|!=
literal|null
condition|)
block|{
name|instrumentation
operator|.
name|recordsDeleted
argument_list|(
name|count
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Initiate the rename operation by creating the tracker for the filesystem    * to keep up to date with state changes in the S3A bucket.    * @param storeContext store context.    * @param source source path    * @param sourceStatus status of the source file/dir    * @param dest destination path.    * @return the rename tracker    */
annotation|@
name|Override
DECL|method|initiateRenameOperation ( final StoreContext storeContext, final Path source, final S3AFileStatus sourceStatus, final Path dest)
specifier|public
name|RenameTracker
name|initiateRenameOperation
parameter_list|(
specifier|final
name|StoreContext
name|storeContext
parameter_list|,
specifier|final
name|Path
name|source
parameter_list|,
specifier|final
name|S3AFileStatus
name|sourceStatus
parameter_list|,
specifier|final
name|Path
name|dest
parameter_list|)
block|{
return|return
operator|new
name|ProgressiveRenameTracker
argument_list|(
name|storeContext
argument_list|,
name|this
argument_list|,
name|source
argument_list|,
name|dest
argument_list|,
operator|new
name|AncestorState
argument_list|(
name|this
argument_list|,
name|BulkOperationState
operator|.
name|OperationType
operator|.
name|Rename
argument_list|,
name|dest
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
DECL|method|initiateBulkWrite ( final BulkOperationState.OperationType operation, final Path dest)
specifier|public
name|AncestorState
name|initiateBulkWrite
parameter_list|(
specifier|final
name|BulkOperationState
operator|.
name|OperationType
name|operation
parameter_list|,
specifier|final
name|Path
name|dest
parameter_list|)
block|{
return|return
operator|new
name|AncestorState
argument_list|(
name|this
argument_list|,
name|operation
argument_list|,
name|dest
argument_list|)
return|;
block|}
annotation|@
name|Override
DECL|method|setTtlTimeProvider (ITtlTimeProvider ttlTimeProvider)
specifier|public
name|void
name|setTtlTimeProvider
parameter_list|(
name|ITtlTimeProvider
name|ttlTimeProvider
parameter_list|)
block|{
name|this
operator|.
name|ttlTimeProvider
operator|=
name|ttlTimeProvider
expr_stmt|;
block|}
comment|/**    * Extract a time provider from the argument or fall back to the    * one in the constructor.    * @param ttlTp nullable time source passed in as an argument.    * @return a non-null time source.    */
DECL|method|extractTimeProvider ( @ullable ITtlTimeProvider ttlTp)
specifier|private
name|ITtlTimeProvider
name|extractTimeProvider
parameter_list|(
annotation|@
name|Nullable
name|ITtlTimeProvider
name|ttlTp
parameter_list|)
block|{
return|return
name|ttlTp
operator|!=
literal|null
condition|?
name|ttlTp
else|:
name|this
operator|.
name|ttlTimeProvider
return|;
block|}
comment|/**    * Username.    * @return the current username    */
DECL|method|getUsername ()
name|String
name|getUsername
parameter_list|()
block|{
return|return
name|username
return|;
block|}
comment|/**    * Log a PUT into the operations log at debug level.    * @param state optional ancestor state.    * @param items items which have been PUT    */
DECL|method|logPut ( @ullable AncestorState state, Item[] items)
specifier|private
specifier|static
name|void
name|logPut
parameter_list|(
annotation|@
name|Nullable
name|AncestorState
name|state
parameter_list|,
name|Item
index|[]
name|items
parameter_list|)
block|{
if|if
condition|(
name|OPERATIONS_LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
comment|// log the operations
name|String
name|stateStr
init|=
name|AncestorState
operator|.
name|stateAsString
argument_list|(
name|state
argument_list|)
decl_stmt|;
for|for
control|(
name|Item
name|item
range|:
name|items
control|)
block|{
name|boolean
name|tombstone
init|=
operator|!
name|itemExists
argument_list|(
name|item
argument_list|)
decl_stmt|;
name|OPERATIONS_LOG
operator|.
name|debug
argument_list|(
literal|"{} {} {}"
argument_list|,
name|stateStr
argument_list|,
name|tombstone
condition|?
literal|"TOMBSTONE"
else|:
literal|"PUT"
argument_list|,
name|itemPrimaryKeyToString
argument_list|(
name|item
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Log a PUT into the operations log at debug level.    * @param state optional ancestor state.    * @param item item PUT.    */
DECL|method|logPut ( @ullable AncestorState state, Item item)
specifier|private
specifier|static
name|void
name|logPut
parameter_list|(
annotation|@
name|Nullable
name|AncestorState
name|state
parameter_list|,
name|Item
name|item
parameter_list|)
block|{
if|if
condition|(
name|OPERATIONS_LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
comment|// log the operations
name|logPut
argument_list|(
name|state
argument_list|,
operator|new
name|Item
index|[]
block|{
name|item
block|}
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Log a DELETE into the operations log at debug level.    * @param state optional ancestor state.    * @param keysDeleted keys which were deleted.    */
DECL|method|logDelete ( @ullable AncestorState state, PrimaryKey[] keysDeleted)
specifier|private
specifier|static
name|void
name|logDelete
parameter_list|(
annotation|@
name|Nullable
name|AncestorState
name|state
parameter_list|,
name|PrimaryKey
index|[]
name|keysDeleted
parameter_list|)
block|{
if|if
condition|(
name|OPERATIONS_LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
comment|// log the operations
name|String
name|stateStr
init|=
name|AncestorState
operator|.
name|stateAsString
argument_list|(
name|state
argument_list|)
decl_stmt|;
for|for
control|(
name|PrimaryKey
name|key
range|:
name|keysDeleted
control|)
block|{
name|OPERATIONS_LOG
operator|.
name|debug
argument_list|(
literal|"{} DELETE {}"
argument_list|,
name|stateStr
argument_list|,
name|primaryKeyToString
argument_list|(
name|key
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Log a DELETE into the operations log at debug level.    * @param state optional ancestor state.    * @param key Deleted key    */
DECL|method|logDelete ( @ullable AncestorState state, PrimaryKey key)
specifier|private
specifier|static
name|void
name|logDelete
parameter_list|(
annotation|@
name|Nullable
name|AncestorState
name|state
parameter_list|,
name|PrimaryKey
name|key
parameter_list|)
block|{
if|if
condition|(
name|OPERATIONS_LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|logDelete
argument_list|(
name|state
argument_list|,
operator|new
name|PrimaryKey
index|[]
block|{
name|key
block|}
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Get the move state passed in; create a new one if needed.    * @param state state.    * @param operation the type of the operation to use if the state is created.    * @return the cast or created state.    */
DECL|method|extractOrCreate (@ullable BulkOperationState state, BulkOperationState.OperationType operation)
specifier|private
name|AncestorState
name|extractOrCreate
parameter_list|(
annotation|@
name|Nullable
name|BulkOperationState
name|state
parameter_list|,
name|BulkOperationState
operator|.
name|OperationType
name|operation
parameter_list|)
block|{
if|if
condition|(
name|state
operator|!=
literal|null
condition|)
block|{
return|return
operator|(
name|AncestorState
operator|)
name|state
return|;
block|}
else|else
block|{
return|return
operator|new
name|AncestorState
argument_list|(
name|this
argument_list|,
name|operation
argument_list|,
literal|null
argument_list|)
return|;
block|}
block|}
comment|/**    * This tracks all the ancestors created,    * across multiple move/write operations.    * This is to avoid duplicate creation of ancestors during bulk commits    * and rename operations managed by a rename tracker.    */
annotation|@
name|VisibleForTesting
DECL|class|AncestorState
specifier|static
specifier|final
class|class
name|AncestorState
extends|extends
name|BulkOperationState
block|{
comment|/**      * Counter of IDs issued.      */
DECL|field|ID_COUNTER
specifier|private
specifier|static
specifier|final
name|AtomicLong
name|ID_COUNTER
init|=
operator|new
name|AtomicLong
argument_list|(
literal|0
argument_list|)
decl_stmt|;
comment|/** Owning store. */
DECL|field|store
specifier|private
specifier|final
name|DynamoDBMetadataStore
name|store
decl_stmt|;
comment|/** The ID of the state; for logging. */
DECL|field|id
specifier|private
specifier|final
name|long
name|id
decl_stmt|;
comment|/**      * Map of ancestors.      */
DECL|field|ancestry
specifier|private
specifier|final
name|Map
argument_list|<
name|Path
argument_list|,
name|DDBPathMetadata
argument_list|>
name|ancestry
init|=
operator|new
name|HashMap
argument_list|<>
argument_list|()
decl_stmt|;
comment|/**      * Destination path.      */
DECL|field|dest
specifier|private
specifier|final
name|Path
name|dest
decl_stmt|;
comment|/**      * Create the state.      * @param store the store, for use in validation.      * If null: no validation (test only operation)      * @param operation the type of the operation.      * @param dest destination path.      */
DECL|method|AncestorState ( @ullable final DynamoDBMetadataStore store, final OperationType operation, @Nullable final Path dest)
name|AncestorState
parameter_list|(
annotation|@
name|Nullable
specifier|final
name|DynamoDBMetadataStore
name|store
parameter_list|,
specifier|final
name|OperationType
name|operation
parameter_list|,
annotation|@
name|Nullable
specifier|final
name|Path
name|dest
parameter_list|)
block|{
name|super
argument_list|(
name|operation
argument_list|)
expr_stmt|;
name|this
operator|.
name|store
operator|=
name|store
expr_stmt|;
name|this
operator|.
name|dest
operator|=
name|dest
expr_stmt|;
name|this
operator|.
name|id
operator|=
name|ID_COUNTER
operator|.
name|addAndGet
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
DECL|method|size ()
name|int
name|size
parameter_list|()
block|{
return|return
name|ancestry
operator|.
name|size
argument_list|()
return|;
block|}
DECL|method|getDest ()
specifier|public
name|Path
name|getDest
parameter_list|()
block|{
return|return
name|dest
return|;
block|}
DECL|method|getId ()
name|long
name|getId
parameter_list|()
block|{
return|return
name|id
return|;
block|}
annotation|@
name|Override
DECL|method|toString ()
specifier|public
name|String
name|toString
parameter_list|()
block|{
specifier|final
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
literal|"AncestorState{"
argument_list|)
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"operation="
argument_list|)
operator|.
name|append
argument_list|(
name|getOperation
argument_list|()
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"id="
argument_list|)
operator|.
name|append
argument_list|(
name|id
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"; dest="
argument_list|)
operator|.
name|append
argument_list|(
name|dest
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"; size="
argument_list|)
operator|.
name|append
argument_list|(
name|size
argument_list|()
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"; paths={"
argument_list|)
operator|.
name|append
argument_list|(
name|StringUtils
operator|.
name|join
argument_list|(
name|ancestry
operator|.
name|keySet
argument_list|()
argument_list|,
literal|" "
argument_list|)
argument_list|)
operator|.
name|append
argument_list|(
literal|'}'
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|'}'
argument_list|)
expr_stmt|;
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**      * Does the ancestor state contain a path?      * @param p path to check      * @return true if the state has an entry      */
DECL|method|contains (Path p)
name|boolean
name|contains
parameter_list|(
name|Path
name|p
parameter_list|)
block|{
return|return
name|get
argument_list|(
name|p
argument_list|)
operator|!=
literal|null
return|;
block|}
DECL|method|put (Path p, DDBPathMetadata md)
name|DDBPathMetadata
name|put
parameter_list|(
name|Path
name|p
parameter_list|,
name|DDBPathMetadata
name|md
parameter_list|)
block|{
return|return
name|ancestry
operator|.
name|put
argument_list|(
name|p
argument_list|,
name|md
argument_list|)
return|;
block|}
DECL|method|put (DDBPathMetadata md)
name|DDBPathMetadata
name|put
parameter_list|(
name|DDBPathMetadata
name|md
parameter_list|)
block|{
return|return
name|ancestry
operator|.
name|put
argument_list|(
name|md
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|md
argument_list|)
return|;
block|}
DECL|method|get (Path p)
name|DDBPathMetadata
name|get
parameter_list|(
name|Path
name|p
parameter_list|)
block|{
return|return
name|ancestry
operator|.
name|get
argument_list|(
name|p
argument_list|)
return|;
block|}
comment|/**      * Find an entry in the ancestor state, warning and optionally      * raising an exception if there is a file at the path.      * @param path path to look up      * @param failOnFile fail if a file was found.      * @return true iff a directory was found in the ancestor state.      * @throws PathIOException if there was a file at the path.      */
DECL|method|findEntry ( final Path path, final boolean failOnFile)
name|boolean
name|findEntry
parameter_list|(
specifier|final
name|Path
name|path
parameter_list|,
specifier|final
name|boolean
name|failOnFile
parameter_list|)
throws|throws
name|PathIOException
block|{
specifier|final
name|DDBPathMetadata
name|ancestor
init|=
name|get
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|ancestor
operator|!=
literal|null
condition|)
block|{
comment|// there's an entry in the ancestor state
if|if
condition|(
operator|!
name|ancestor
operator|.
name|getFileStatus
argument_list|()
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
comment|// but: its a file, which means this update is now inconsistent.
specifier|final
name|String
name|message
init|=
name|E_INCONSISTENT_UPDATE
operator|+
literal|" entry is "
operator|+
name|ancestor
operator|.
name|getFileStatus
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|message
argument_list|)
expr_stmt|;
if|if
condition|(
name|failOnFile
condition|)
block|{
comment|// errors trigger failure
throw|throw
operator|new
name|PathIOException
argument_list|(
name|path
operator|.
name|toString
argument_list|()
argument_list|,
name|message
argument_list|)
throw|;
block|}
block|}
return|return
literal|true
return|;
block|}
else|else
block|{
return|return
literal|false
return|;
block|}
block|}
comment|/**      * If debug logging is enabled, this does an audit of the store state.      * it only logs this; the error messages are created so as they could      * be turned into exception messages.      * Audit failures aren't being turned into IOEs is that      * rename operations delete the source entry and that ends up in the      * ancestor state as present      * @throws IOException failure      */
annotation|@
name|Override
DECL|method|close ()
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
operator|&&
name|store
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Auditing {}"
argument_list|,
name|stateAsString
argument_list|(
name|this
argument_list|)
argument_list|)
expr_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Path
argument_list|,
name|DDBPathMetadata
argument_list|>
name|entry
range|:
name|ancestry
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|Path
name|path
init|=
name|entry
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|DDBPathMetadata
name|expected
init|=
name|entry
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|expected
operator|.
name|isDeleted
argument_list|()
condition|)
block|{
comment|// file was deleted in bulk op; we don't care about it
comment|// any more
continue|continue;
block|}
name|DDBPathMetadata
name|actual
decl_stmt|;
try|try
block|{
name|actual
operator|=
name|store
operator|.
name|get
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Retrieving {}"
argument_list|,
name|path
argument_list|,
name|e
argument_list|)
expr_stmt|;
comment|// this is for debug; don't be ambitious
return|return;
block|}
if|if
condition|(
name|actual
operator|==
literal|null
operator|||
name|actual
operator|.
name|isDeleted
argument_list|()
condition|)
block|{
name|String
name|message
init|=
literal|"Metastore entry for path "
operator|+
name|path
operator|+
literal|" deleted during bulk "
operator|+
name|getOperation
argument_list|()
operator|+
literal|" operation"
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
name|message
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|actual
operator|.
name|getFileStatus
argument_list|()
operator|.
name|isDirectory
argument_list|()
operator|!=
name|expected
operator|.
name|getFileStatus
argument_list|()
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
comment|// the type of the entry has changed
name|String
name|message
init|=
literal|"Metastore entry for path "
operator|+
name|path
operator|+
literal|" changed during bulk "
operator|+
name|getOperation
argument_list|()
operator|+
literal|" operation"
operator|+
literal|" from "
operator|+
name|expected
operator|+
literal|" to "
operator|+
name|actual
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
name|message
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
comment|/**      * Create a string from the state including operation and ID.      * @param state state to use -may be null      * @return a string for logging.      */
DECL|method|stateAsString (@ullable AncestorState state)
specifier|private
specifier|static
name|String
name|stateAsString
parameter_list|(
annotation|@
name|Nullable
name|AncestorState
name|state
parameter_list|)
block|{
name|String
name|stateStr
decl_stmt|;
if|if
condition|(
name|state
operator|!=
literal|null
condition|)
block|{
name|stateStr
operator|=
name|String
operator|.
name|format
argument_list|(
literal|"#(%s-%04d)"
argument_list|,
name|state
operator|.
name|getOperation
argument_list|()
argument_list|,
name|state
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|stateStr
operator|=
literal|"#()"
expr_stmt|;
block|}
return|return
name|stateStr
return|;
block|}
block|}
DECL|method|getTableHandler ()
specifier|protected
name|DynamoDBMetadataStoreTableManager
name|getTableHandler
parameter_list|()
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|tableHandler
argument_list|,
literal|"Not initialized"
argument_list|)
expr_stmt|;
return|return
name|tableHandler
return|;
block|}
block|}
end_class

end_unit

