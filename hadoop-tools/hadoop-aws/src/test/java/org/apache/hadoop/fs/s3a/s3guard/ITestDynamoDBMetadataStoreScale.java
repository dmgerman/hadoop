begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
DECL|package|org.apache.hadoop.fs.s3a.s3guard
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|s3guard
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|javax
operator|.
name|annotation
operator|.
name|Nullable
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|document
operator|.
name|DynamoDB
import|;
end_import

begin_import
import|import
name|com
operator|.
name|amazonaws
operator|.
name|services
operator|.
name|dynamodbv2
operator|.
name|model
operator|.
name|ProvisionedThroughputDescription
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|Test
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|scale
operator|.
name|AbstractITestS3AMetadataStoreScale
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|s3guard
operator|.
name|MetadataStoreTestBase
operator|.
name|basicFileStatus
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|s3a
operator|.
name|Constants
operator|.
name|*
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|junit
operator|.
name|Assume
operator|.
name|*
import|;
end_import

begin_comment
comment|/**  * Scale test for DynamoDBMetadataStore.  */
end_comment

begin_class
DECL|class|ITestDynamoDBMetadataStoreScale
specifier|public
class|class
name|ITestDynamoDBMetadataStoreScale
extends|extends
name|AbstractITestS3AMetadataStoreScale
block|{
DECL|field|BATCH_SIZE
specifier|private
specifier|static
specifier|final
name|long
name|BATCH_SIZE
init|=
literal|25
decl_stmt|;
DECL|field|SMALL_IO_UNITS
specifier|private
specifier|static
specifier|final
name|long
name|SMALL_IO_UNITS
init|=
name|BATCH_SIZE
operator|/
literal|4
decl_stmt|;
annotation|@
name|Override
DECL|method|createMetadataStore ()
specifier|public
name|MetadataStore
name|createMetadataStore
parameter_list|()
throws|throws
name|IOException
block|{
name|Configuration
name|conf
init|=
name|getFileSystem
argument_list|()
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|String
name|ddbTable
init|=
name|conf
operator|.
name|get
argument_list|(
name|S3GUARD_DDB_TABLE_NAME_KEY
argument_list|)
decl_stmt|;
name|assumeNotNull
argument_list|(
literal|"DynamoDB table is configured"
argument_list|,
name|ddbTable
argument_list|)
expr_stmt|;
name|String
name|ddbEndpoint
init|=
name|conf
operator|.
name|get
argument_list|(
name|S3GUARD_DDB_REGION_KEY
argument_list|)
decl_stmt|;
name|assumeNotNull
argument_list|(
literal|"DynamoDB endpoint is configured"
argument_list|,
name|ddbEndpoint
argument_list|)
expr_stmt|;
name|DynamoDBMetadataStore
name|ms
init|=
operator|new
name|DynamoDBMetadataStore
argument_list|()
decl_stmt|;
name|ms
operator|.
name|initialize
argument_list|(
name|getFileSystem
argument_list|()
operator|.
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|ms
return|;
block|}
comment|/**    * Though the AWS SDK claims in documentation to handle retries and    * exponential backoff, we have witnessed    * com.amazonaws...dynamodbv2.model.ProvisionedThroughputExceededException    * (Status Code: 400; Error Code: ProvisionedThroughputExceededException)    * Hypothesis:    * Happens when the size of a batched write is bigger than the number of    * provisioned write units.  This test ensures we handle the case    * correctly, retrying w/ smaller batch instead of surfacing exceptions.    */
annotation|@
name|Test
DECL|method|testBatchedWriteExceedsProvisioned ()
specifier|public
name|void
name|testBatchedWriteExceedsProvisioned
parameter_list|()
throws|throws
name|Exception
block|{
specifier|final
name|long
name|iterations
init|=
literal|5
decl_stmt|;
name|boolean
name|isProvisionedChanged
decl_stmt|;
name|List
argument_list|<
name|PathMetadata
argument_list|>
name|toCleanup
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
comment|// Fail if someone changes a constant we depend on
name|assertTrue
argument_list|(
literal|"Maximum batch size must big enough to run this test"
argument_list|,
name|S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT
operator|>=
name|BATCH_SIZE
argument_list|)
expr_stmt|;
try|try
init|(
name|DynamoDBMetadataStore
name|ddbms
init|=
operator|(
name|DynamoDBMetadataStore
operator|)
name|createMetadataStore
argument_list|()
init|)
block|{
name|DynamoDB
name|ddb
init|=
name|ddbms
operator|.
name|getDynamoDB
argument_list|()
decl_stmt|;
name|String
name|tableName
init|=
name|ddbms
operator|.
name|getTable
argument_list|()
operator|.
name|getTableName
argument_list|()
decl_stmt|;
specifier|final
name|ProvisionedThroughputDescription
name|existing
init|=
name|ddb
operator|.
name|getTable
argument_list|(
name|tableName
argument_list|)
operator|.
name|describe
argument_list|()
operator|.
name|getProvisionedThroughput
argument_list|()
decl_stmt|;
comment|// If you set the same provisioned I/O as already set it throws an
comment|// exception, avoid that.
name|isProvisionedChanged
operator|=
operator|(
name|existing
operator|.
name|getReadCapacityUnits
argument_list|()
operator|!=
name|SMALL_IO_UNITS
operator|||
name|existing
operator|.
name|getWriteCapacityUnits
argument_list|()
operator|!=
name|SMALL_IO_UNITS
operator|)
expr_stmt|;
if|if
condition|(
name|isProvisionedChanged
condition|)
block|{
comment|// Set low provisioned I/O for dynamodb
name|describe
argument_list|(
literal|"Provisioning dynamo tbl %s read/write -> %d/%d"
argument_list|,
name|tableName
argument_list|,
name|SMALL_IO_UNITS
argument_list|,
name|SMALL_IO_UNITS
argument_list|)
expr_stmt|;
comment|// Blocks to ensure table is back to ready state before we proceed
name|ddbms
operator|.
name|provisionTableBlocking
argument_list|(
name|SMALL_IO_UNITS
argument_list|,
name|SMALL_IO_UNITS
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|describe
argument_list|(
literal|"Skipping provisioning table I/O, already %d/%d"
argument_list|,
name|SMALL_IO_UNITS
argument_list|,
name|SMALL_IO_UNITS
argument_list|)
expr_stmt|;
block|}
try|try
block|{
comment|// We know the dynamodb metadata store will expand a put of a path
comment|// of depth N into a batch of N writes (all ancestors are written
comment|// separately up to the root).  (Ab)use this for an easy way to write
comment|// a batch of stuff that is bigger than the provisioned write units
try|try
block|{
name|describe
argument_list|(
literal|"Running %d iterations of batched put, size %d"
argument_list|,
name|iterations
argument_list|,
name|BATCH_SIZE
argument_list|)
expr_stmt|;
name|long
name|pruneItems
init|=
literal|0
decl_stmt|;
for|for
control|(
name|long
name|i
init|=
literal|0
init|;
name|i
operator|<
name|iterations
condition|;
name|i
operator|++
control|)
block|{
name|Path
name|longPath
init|=
name|pathOfDepth
argument_list|(
name|BATCH_SIZE
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|i
argument_list|)
argument_list|)
decl_stmt|;
name|FileStatus
name|status
init|=
name|basicFileStatus
argument_list|(
name|longPath
argument_list|,
literal|0
argument_list|,
literal|false
argument_list|,
literal|12345
argument_list|,
literal|12345
argument_list|)
decl_stmt|;
name|PathMetadata
name|pm
init|=
operator|new
name|PathMetadata
argument_list|(
name|status
argument_list|)
decl_stmt|;
name|ddbms
operator|.
name|put
argument_list|(
name|pm
argument_list|)
expr_stmt|;
name|toCleanup
operator|.
name|add
argument_list|(
name|pm
argument_list|)
expr_stmt|;
name|pruneItems
operator|++
expr_stmt|;
comment|// Having hard time reproducing Exceeded exception with put, also
comment|// try occasional prune, which was the only stack trace I've seen
comment|// (on JIRA)
if|if
condition|(
name|pruneItems
operator|==
name|BATCH_SIZE
condition|)
block|{
name|describe
argument_list|(
literal|"pruning files"
argument_list|)
expr_stmt|;
name|ddbms
operator|.
name|prune
argument_list|(
name|Long
operator|.
name|MAX_VALUE
comment|/* all files */
argument_list|)
expr_stmt|;
name|pruneItems
operator|=
literal|0
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|describe
argument_list|(
literal|"Cleaning up table %s"
argument_list|,
name|tableName
argument_list|)
expr_stmt|;
for|for
control|(
name|PathMetadata
name|pm
range|:
name|toCleanup
control|)
block|{
name|cleanupMetadata
argument_list|(
name|ddbms
argument_list|,
name|pm
argument_list|)
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|isProvisionedChanged
condition|)
block|{
name|long
name|write
init|=
name|existing
operator|.
name|getWriteCapacityUnits
argument_list|()
decl_stmt|;
name|long
name|read
init|=
name|existing
operator|.
name|getReadCapacityUnits
argument_list|()
decl_stmt|;
name|describe
argument_list|(
literal|"Restoring dynamo tbl %s read/write -> %d/%d"
argument_list|,
name|tableName
argument_list|,
name|read
argument_list|,
name|write
argument_list|)
expr_stmt|;
name|ddbms
operator|.
name|provisionTableBlocking
argument_list|(
name|existing
operator|.
name|getReadCapacityUnits
argument_list|()
argument_list|,
name|existing
operator|.
name|getWriteCapacityUnits
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// Attempt do delete metadata, suppressing any errors
DECL|method|cleanupMetadata (MetadataStore ms, PathMetadata pm)
specifier|private
name|void
name|cleanupMetadata
parameter_list|(
name|MetadataStore
name|ms
parameter_list|,
name|PathMetadata
name|pm
parameter_list|)
block|{
try|try
block|{
name|ms
operator|.
name|forgetMetadata
argument_list|(
name|pm
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
comment|// Ignore.
block|}
block|}
DECL|method|pathOfDepth (long n, @Nullable String fileSuffix)
specifier|private
name|Path
name|pathOfDepth
parameter_list|(
name|long
name|n
parameter_list|,
annotation|@
name|Nullable
name|String
name|fileSuffix
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|long
name|i
init|=
literal|0
init|;
name|i
operator|<
name|n
condition|;
name|i
operator|++
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|i
operator|==
literal|0
condition|?
literal|"/"
operator|+
name|this
operator|.
name|getClass
argument_list|()
operator|.
name|getSimpleName
argument_list|()
else|:
literal|"lvl"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|i
argument_list|)
expr_stmt|;
if|if
condition|(
name|i
operator|==
name|n
operator|-
literal|1
operator|&&
name|fileSuffix
operator|!=
literal|null
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|fileSuffix
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|append
argument_list|(
literal|"/"
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|getFileSystem
argument_list|()
operator|.
name|getUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|sb
operator|.
name|toString
argument_list|()
argument_list|)
return|;
block|}
block|}
end_class

end_unit

