begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
DECL|package|org.apache.hadoop.mapreduce.lib.input
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|lib
operator|.
name|input
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|classification
operator|.
name|InterfaceAudience
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|classification
operator|.
name|InterfaceStability
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|LocatedFileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|BlockLocation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|compress
operator|.
name|CompressionCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|compress
operator|.
name|CompressionCodecFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|compress
operator|.
name|SplittableCompressionCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|RecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|TaskAttemptContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|net
operator|.
name|NodeBase
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|net
operator|.
name|NetworkTopology
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|HashMultiset
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Multiset
import|;
end_import

begin_comment
comment|/**  * An abstract {@link InputFormat} that returns {@link CombineFileSplit}'s in   * {@link InputFormat#getSplits(JobContext)} method.   *   * Splits are constructed from the files under the input paths.   * A split cannot have files from different pools.  * Each split returned may contain blocks from different files.  * If a maxSplitSize is specified, then blocks on the same node are  * combined to form a single split. Blocks that are left over are  * then combined with other blocks in the same rack.   * If maxSplitSize is not specified, then blocks from the same rack  * are combined in a single split; no attempt is made to create  * node-local splits.  * If the maxSplitSize is equal to the block size, then this class  * is similar to the default splitting behavior in Hadoop: each  * block is a locally processed split.  * Subclasses implement   * {@link InputFormat#createRecordReader(InputSplit, TaskAttemptContext)}  * to construct<code>RecordReader</code>'s for   *<code>CombineFileSplit</code>'s.  *   * @see CombineFileSplit  */
end_comment

begin_class
annotation|@
name|InterfaceAudience
operator|.
name|Public
annotation|@
name|InterfaceStability
operator|.
name|Stable
DECL|class|CombineFileInputFormat
specifier|public
specifier|abstract
class|class
name|CombineFileInputFormat
parameter_list|<
name|K
parameter_list|,
name|V
parameter_list|>
extends|extends
name|FileInputFormat
argument_list|<
name|K
argument_list|,
name|V
argument_list|>
block|{
DECL|field|LOG
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|CombineFileInputFormat
operator|.
name|class
argument_list|)
decl_stmt|;
DECL|field|SPLIT_MINSIZE_PERNODE
specifier|public
specifier|static
specifier|final
name|String
name|SPLIT_MINSIZE_PERNODE
init|=
literal|"mapreduce.input.fileinputformat.split.minsize.per.node"
decl_stmt|;
DECL|field|SPLIT_MINSIZE_PERRACK
specifier|public
specifier|static
specifier|final
name|String
name|SPLIT_MINSIZE_PERRACK
init|=
literal|"mapreduce.input.fileinputformat.split.minsize.per.rack"
decl_stmt|;
comment|// ability to limit the size of a single split
DECL|field|maxSplitSize
specifier|private
name|long
name|maxSplitSize
init|=
literal|0
decl_stmt|;
DECL|field|minSplitSizeNode
specifier|private
name|long
name|minSplitSizeNode
init|=
literal|0
decl_stmt|;
DECL|field|minSplitSizeRack
specifier|private
name|long
name|minSplitSizeRack
init|=
literal|0
decl_stmt|;
comment|// A pool of input paths filters. A split cannot have blocks from files
comment|// across multiple pools.
DECL|field|pools
specifier|private
name|ArrayList
argument_list|<
name|MultiPathFilter
argument_list|>
name|pools
init|=
operator|new
name|ArrayList
argument_list|<
name|MultiPathFilter
argument_list|>
argument_list|()
decl_stmt|;
comment|// mapping from a rack name to the set of Nodes in the rack
DECL|field|rackToNodes
specifier|private
name|HashMap
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|String
argument_list|>
argument_list|>
name|rackToNodes
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
comment|/**    * Specify the maximum size (in bytes) of each split. Each split is    * approximately equal to the specified size.    */
DECL|method|setMaxSplitSize (long maxSplitSize)
specifier|protected
name|void
name|setMaxSplitSize
parameter_list|(
name|long
name|maxSplitSize
parameter_list|)
block|{
name|this
operator|.
name|maxSplitSize
operator|=
name|maxSplitSize
expr_stmt|;
block|}
comment|/**    * Specify the minimum size (in bytes) of each split per node.    * This applies to data that is left over after combining data on a single    * node into splits that are of maximum size specified by maxSplitSize.    * This leftover data will be combined into its own split if its size    * exceeds minSplitSizeNode.    */
DECL|method|setMinSplitSizeNode (long minSplitSizeNode)
specifier|protected
name|void
name|setMinSplitSizeNode
parameter_list|(
name|long
name|minSplitSizeNode
parameter_list|)
block|{
name|this
operator|.
name|minSplitSizeNode
operator|=
name|minSplitSizeNode
expr_stmt|;
block|}
comment|/**    * Specify the minimum size (in bytes) of each split per rack.    * This applies to data that is left over after combining data on a single    * rack into splits that are of maximum size specified by maxSplitSize.    * This leftover data will be combined into its own split if its size    * exceeds minSplitSizeRack.    */
DECL|method|setMinSplitSizeRack (long minSplitSizeRack)
specifier|protected
name|void
name|setMinSplitSizeRack
parameter_list|(
name|long
name|minSplitSizeRack
parameter_list|)
block|{
name|this
operator|.
name|minSplitSizeRack
operator|=
name|minSplitSizeRack
expr_stmt|;
block|}
comment|/**    * Create a new pool and add the filters to it.    * A split cannot have files from different pools.    */
DECL|method|createPool (List<PathFilter> filters)
specifier|protected
name|void
name|createPool
parameter_list|(
name|List
argument_list|<
name|PathFilter
argument_list|>
name|filters
parameter_list|)
block|{
name|pools
operator|.
name|add
argument_list|(
operator|new
name|MultiPathFilter
argument_list|(
name|filters
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**    * Create a new pool and add the filters to it.     * A pathname can satisfy any one of the specified filters.    * A split cannot have files from different pools.    */
DECL|method|createPool (PathFilter... filters)
specifier|protected
name|void
name|createPool
parameter_list|(
name|PathFilter
modifier|...
name|filters
parameter_list|)
block|{
name|MultiPathFilter
name|multi
init|=
operator|new
name|MultiPathFilter
argument_list|()
decl_stmt|;
for|for
control|(
name|PathFilter
name|f
range|:
name|filters
control|)
block|{
name|multi
operator|.
name|add
argument_list|(
name|f
argument_list|)
expr_stmt|;
block|}
name|pools
operator|.
name|add
argument_list|(
name|multi
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|isSplitable (JobContext context, Path file)
specifier|protected
name|boolean
name|isSplitable
parameter_list|(
name|JobContext
name|context
parameter_list|,
name|Path
name|file
parameter_list|)
block|{
specifier|final
name|CompressionCodec
name|codec
init|=
operator|new
name|CompressionCodecFactory
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
operator|.
name|getCodec
argument_list|(
name|file
argument_list|)
decl_stmt|;
if|if
condition|(
literal|null
operator|==
name|codec
condition|)
block|{
return|return
literal|true
return|;
block|}
return|return
name|codec
operator|instanceof
name|SplittableCompressionCodec
return|;
block|}
comment|/**    * default constructor    */
DECL|method|CombineFileInputFormat ()
specifier|public
name|CombineFileInputFormat
parameter_list|()
block|{   }
annotation|@
name|Override
DECL|method|getSplits (JobContext job)
specifier|public
name|List
argument_list|<
name|InputSplit
argument_list|>
name|getSplits
parameter_list|(
name|JobContext
name|job
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|minSizeNode
init|=
literal|0
decl_stmt|;
name|long
name|minSizeRack
init|=
literal|0
decl_stmt|;
name|long
name|maxSize
init|=
literal|0
decl_stmt|;
name|Configuration
name|conf
init|=
name|job
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
comment|// the values specified by setxxxSplitSize() takes precedence over the
comment|// values that might have been specified in the config
if|if
condition|(
name|minSplitSizeNode
operator|!=
literal|0
condition|)
block|{
name|minSizeNode
operator|=
name|minSplitSizeNode
expr_stmt|;
block|}
else|else
block|{
name|minSizeNode
operator|=
name|conf
operator|.
name|getLong
argument_list|(
name|SPLIT_MINSIZE_PERNODE
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|minSplitSizeRack
operator|!=
literal|0
condition|)
block|{
name|minSizeRack
operator|=
name|minSplitSizeRack
expr_stmt|;
block|}
else|else
block|{
name|minSizeRack
operator|=
name|conf
operator|.
name|getLong
argument_list|(
name|SPLIT_MINSIZE_PERRACK
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|maxSplitSize
operator|!=
literal|0
condition|)
block|{
name|maxSize
operator|=
name|maxSplitSize
expr_stmt|;
block|}
else|else
block|{
name|maxSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"mapreduce.input.fileinputformat.split.maxsize"
argument_list|,
literal|0
argument_list|)
expr_stmt|;
comment|// If maxSize is not configured, a single split will be generated per
comment|// node.
block|}
if|if
condition|(
name|minSizeNode
operator|!=
literal|0
operator|&&
name|maxSize
operator|!=
literal|0
operator|&&
name|minSizeNode
operator|>
name|maxSize
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Minimum split size pernode "
operator|+
name|minSizeNode
operator|+
literal|" cannot be larger than maximum split size "
operator|+
name|maxSize
argument_list|)
throw|;
block|}
if|if
condition|(
name|minSizeRack
operator|!=
literal|0
operator|&&
name|maxSize
operator|!=
literal|0
operator|&&
name|minSizeRack
operator|>
name|maxSize
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Minimum split size per rack "
operator|+
name|minSizeRack
operator|+
literal|" cannot be larger than maximum split size "
operator|+
name|maxSize
argument_list|)
throw|;
block|}
if|if
condition|(
name|minSizeRack
operator|!=
literal|0
operator|&&
name|minSizeNode
operator|>
name|minSizeRack
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Minimum split size per node "
operator|+
name|minSizeNode
operator|+
literal|" cannot be larger than minimum split "
operator|+
literal|"size per rack "
operator|+
name|minSizeRack
argument_list|)
throw|;
block|}
comment|// all the files in input set
name|List
argument_list|<
name|FileStatus
argument_list|>
name|stats
init|=
name|listStatus
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|InputSplit
argument_list|>
name|splits
init|=
operator|new
name|ArrayList
argument_list|<
name|InputSplit
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|stats
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
name|splits
return|;
block|}
comment|// In one single iteration, process all the paths in a single pool.
comment|// Processing one pool at a time ensures that a split contains paths
comment|// from a single pool only.
for|for
control|(
name|MultiPathFilter
name|onepool
range|:
name|pools
control|)
block|{
name|ArrayList
argument_list|<
name|FileStatus
argument_list|>
name|myPaths
init|=
operator|new
name|ArrayList
argument_list|<
name|FileStatus
argument_list|>
argument_list|()
decl_stmt|;
comment|// pick one input path. If it matches all the filters in a pool,
comment|// add it to the output set
for|for
control|(
name|Iterator
argument_list|<
name|FileStatus
argument_list|>
name|iter
init|=
name|stats
operator|.
name|iterator
argument_list|()
init|;
name|iter
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|FileStatus
name|p
init|=
name|iter
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|onepool
operator|.
name|accept
argument_list|(
name|p
operator|.
name|getPath
argument_list|()
argument_list|)
condition|)
block|{
name|myPaths
operator|.
name|add
argument_list|(
name|p
argument_list|)
expr_stmt|;
comment|// add it to my output set
name|iter
operator|.
name|remove
argument_list|()
expr_stmt|;
block|}
block|}
comment|// create splits for all files in this pool.
name|getMoreSplits
argument_list|(
name|job
argument_list|,
name|myPaths
argument_list|,
name|maxSize
argument_list|,
name|minSizeNode
argument_list|,
name|minSizeRack
argument_list|,
name|splits
argument_list|)
expr_stmt|;
block|}
comment|// create splits for all files that are not in any pool.
name|getMoreSplits
argument_list|(
name|job
argument_list|,
name|stats
argument_list|,
name|maxSize
argument_list|,
name|minSizeNode
argument_list|,
name|minSizeRack
argument_list|,
name|splits
argument_list|)
expr_stmt|;
comment|// free up rackToNodes map
name|rackToNodes
operator|.
name|clear
argument_list|()
expr_stmt|;
return|return
name|splits
return|;
block|}
comment|/**    * Return all the splits in the specified set of paths    */
DECL|method|getMoreSplits (JobContext job, List<FileStatus> stats, long maxSize, long minSizeNode, long minSizeRack, List<InputSplit> splits)
specifier|private
name|void
name|getMoreSplits
parameter_list|(
name|JobContext
name|job
parameter_list|,
name|List
argument_list|<
name|FileStatus
argument_list|>
name|stats
parameter_list|,
name|long
name|maxSize
parameter_list|,
name|long
name|minSizeNode
parameter_list|,
name|long
name|minSizeRack
parameter_list|,
name|List
argument_list|<
name|InputSplit
argument_list|>
name|splits
parameter_list|)
throws|throws
name|IOException
block|{
name|Configuration
name|conf
init|=
name|job
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
comment|// all blocks for all the files in input set
name|OneFileInfo
index|[]
name|files
decl_stmt|;
comment|// mapping from a rack name to the list of blocks it has
name|HashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|rackToBlocks
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
comment|// mapping from a block to the nodes on which it has replicas
name|HashMap
argument_list|<
name|OneBlockInfo
argument_list|,
name|String
index|[]
argument_list|>
name|blockToNodes
init|=
operator|new
name|HashMap
argument_list|<
name|OneBlockInfo
argument_list|,
name|String
index|[]
argument_list|>
argument_list|()
decl_stmt|;
comment|// mapping from a node to the list of blocks that it contains
name|HashMap
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|nodeToBlocks
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|files
operator|=
operator|new
name|OneFileInfo
index|[
name|stats
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
if|if
condition|(
name|stats
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return;
block|}
comment|// populate all the blocks for all files
name|long
name|totLength
init|=
literal|0
decl_stmt|;
name|int
name|i
init|=
literal|0
decl_stmt|;
for|for
control|(
name|FileStatus
name|stat
range|:
name|stats
control|)
block|{
name|files
index|[
name|i
index|]
operator|=
operator|new
name|OneFileInfo
argument_list|(
name|stat
argument_list|,
name|conf
argument_list|,
name|isSplitable
argument_list|(
name|job
argument_list|,
name|stat
operator|.
name|getPath
argument_list|()
argument_list|)
argument_list|,
name|rackToBlocks
argument_list|,
name|blockToNodes
argument_list|,
name|nodeToBlocks
argument_list|,
name|rackToNodes
argument_list|,
name|maxSize
argument_list|)
expr_stmt|;
name|totLength
operator|+=
name|files
index|[
name|i
index|]
operator|.
name|getLength
argument_list|()
expr_stmt|;
block|}
name|createSplits
argument_list|(
name|nodeToBlocks
argument_list|,
name|blockToNodes
argument_list|,
name|rackToBlocks
argument_list|,
name|totLength
argument_list|,
name|maxSize
argument_list|,
name|minSizeNode
argument_list|,
name|minSizeRack
argument_list|,
name|splits
argument_list|)
expr_stmt|;
block|}
comment|/**    * Process all the nodes and create splits that are local to a node.    * Generate one split per node iteration, and walk over nodes multiple times    * to distribute the splits across nodes.    *<p>    * Note: The order of processing the nodes is undetermined because the    * implementation of nodeToBlocks is {@link java.util.HashMap} and its order    * of the entries is undetermined.    * @param nodeToBlocks Mapping from a node to the list of blocks that    *                     it contains.    * @param blockToNodes Mapping from a block to the nodes on which    *                     it has replicas.    * @param rackToBlocks Mapping from a rack name to the list of blocks it has.    * @param totLength Total length of the input files.    * @param maxSize Max size of each split.    *                If set to 0, disable smoothing load.    * @param minSizeNode Minimum split size per node.    * @param minSizeRack Minimum split size per rack.    * @param splits New splits created by this method are added to the list.    */
annotation|@
name|VisibleForTesting
DECL|method|createSplits (Map<String, Set<OneBlockInfo>> nodeToBlocks, Map<OneBlockInfo, String[]> blockToNodes, Map<String, List<OneBlockInfo>> rackToBlocks, long totLength, long maxSize, long minSizeNode, long minSizeRack, List<InputSplit> splits )
name|void
name|createSplits
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|nodeToBlocks
parameter_list|,
name|Map
argument_list|<
name|OneBlockInfo
argument_list|,
name|String
index|[]
argument_list|>
name|blockToNodes
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|rackToBlocks
parameter_list|,
name|long
name|totLength
parameter_list|,
name|long
name|maxSize
parameter_list|,
name|long
name|minSizeNode
parameter_list|,
name|long
name|minSizeRack
parameter_list|,
name|List
argument_list|<
name|InputSplit
argument_list|>
name|splits
parameter_list|)
block|{
name|ArrayList
argument_list|<
name|OneBlockInfo
argument_list|>
name|validBlocks
init|=
operator|new
name|ArrayList
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|()
decl_stmt|;
name|long
name|curSplitSize
init|=
literal|0
decl_stmt|;
name|int
name|totalNodes
init|=
name|nodeToBlocks
operator|.
name|size
argument_list|()
decl_stmt|;
name|long
name|totalLength
init|=
name|totLength
decl_stmt|;
name|Multiset
argument_list|<
name|String
argument_list|>
name|splitsPerNode
init|=
name|HashMultiset
operator|.
name|create
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|completedNodes
init|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
while|while
condition|(
literal|true
condition|)
block|{
for|for
control|(
name|Iterator
argument_list|<
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
argument_list|>
name|iter
init|=
name|nodeToBlocks
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|iter
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|one
init|=
name|iter
operator|.
name|next
argument_list|()
decl_stmt|;
name|String
name|node
init|=
name|one
operator|.
name|getKey
argument_list|()
decl_stmt|;
comment|// Skip the node if it has previously been marked as completed.
if|if
condition|(
name|completedNodes
operator|.
name|contains
argument_list|(
name|node
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|Set
argument_list|<
name|OneBlockInfo
argument_list|>
name|blocksInCurrentNode
init|=
name|one
operator|.
name|getValue
argument_list|()
decl_stmt|;
comment|// for each block, copy it into validBlocks. Delete it from
comment|// blockToNodes so that the same block does not appear in
comment|// two different splits.
name|Iterator
argument_list|<
name|OneBlockInfo
argument_list|>
name|oneBlockIter
init|=
name|blocksInCurrentNode
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|oneBlockIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|OneBlockInfo
name|oneblock
init|=
name|oneBlockIter
operator|.
name|next
argument_list|()
decl_stmt|;
comment|// Remove all blocks which may already have been assigned to other
comment|// splits.
if|if
condition|(
operator|!
name|blockToNodes
operator|.
name|containsKey
argument_list|(
name|oneblock
argument_list|)
condition|)
block|{
name|oneBlockIter
operator|.
name|remove
argument_list|()
expr_stmt|;
continue|continue;
block|}
name|validBlocks
operator|.
name|add
argument_list|(
name|oneblock
argument_list|)
expr_stmt|;
name|blockToNodes
operator|.
name|remove
argument_list|(
name|oneblock
argument_list|)
expr_stmt|;
name|curSplitSize
operator|+=
name|oneblock
operator|.
name|length
expr_stmt|;
comment|// if the accumulated split size exceeds the maximum, then
comment|// create this split.
if|if
condition|(
name|maxSize
operator|!=
literal|0
operator|&&
name|curSplitSize
operator|>=
name|maxSize
condition|)
block|{
comment|// create an input split and add it to the splits array
name|addCreatedSplit
argument_list|(
name|splits
argument_list|,
name|Collections
operator|.
name|singleton
argument_list|(
name|node
argument_list|)
argument_list|,
name|validBlocks
argument_list|)
expr_stmt|;
name|totalLength
operator|-=
name|curSplitSize
expr_stmt|;
name|curSplitSize
operator|=
literal|0
expr_stmt|;
name|splitsPerNode
operator|.
name|add
argument_list|(
name|node
argument_list|)
expr_stmt|;
comment|// Remove entries from blocksInNode so that we don't walk these
comment|// again.
name|blocksInCurrentNode
operator|.
name|removeAll
argument_list|(
name|validBlocks
argument_list|)
expr_stmt|;
name|validBlocks
operator|.
name|clear
argument_list|()
expr_stmt|;
comment|// Done creating a single split for this node. Move on to the next
comment|// node so that splits are distributed across nodes.
break|break;
block|}
block|}
if|if
condition|(
name|validBlocks
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
comment|// This implies that the last few blocks (or all in case maxSize=0)
comment|// were not part of a split. The node is complete.
comment|// if there were any blocks left over and their combined size is
comment|// larger than minSplitNode, then combine them into one split.
comment|// Otherwise add them back to the unprocessed pool. It is likely
comment|// that they will be combined with other blocks from the
comment|// same rack later on.
comment|// This condition also kicks in when max split size is not set. All
comment|// blocks on a node will be grouped together into a single split.
if|if
condition|(
name|minSizeNode
operator|!=
literal|0
operator|&&
name|curSplitSize
operator|>=
name|minSizeNode
operator|&&
name|splitsPerNode
operator|.
name|count
argument_list|(
name|node
argument_list|)
operator|==
literal|0
condition|)
block|{
comment|// haven't created any split on this machine. so its ok to add a
comment|// smaller one for parallelism. Otherwise group it in the rack for
comment|// balanced size create an input split and add it to the splits
comment|// array
name|addCreatedSplit
argument_list|(
name|splits
argument_list|,
name|Collections
operator|.
name|singleton
argument_list|(
name|node
argument_list|)
argument_list|,
name|validBlocks
argument_list|)
expr_stmt|;
name|totalLength
operator|-=
name|curSplitSize
expr_stmt|;
name|splitsPerNode
operator|.
name|add
argument_list|(
name|node
argument_list|)
expr_stmt|;
comment|// Remove entries from blocksInNode so that we don't walk this again.
name|blocksInCurrentNode
operator|.
name|removeAll
argument_list|(
name|validBlocks
argument_list|)
expr_stmt|;
comment|// The node is done. This was the last set of blocks for this node.
block|}
else|else
block|{
comment|// Put the unplaced blocks back into the pool for later rack-allocation.
for|for
control|(
name|OneBlockInfo
name|oneblock
range|:
name|validBlocks
control|)
block|{
name|blockToNodes
operator|.
name|put
argument_list|(
name|oneblock
argument_list|,
name|oneblock
operator|.
name|hosts
argument_list|)
expr_stmt|;
block|}
block|}
name|validBlocks
operator|.
name|clear
argument_list|()
expr_stmt|;
name|curSplitSize
operator|=
literal|0
expr_stmt|;
name|completedNodes
operator|.
name|add
argument_list|(
name|node
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// No in-flight blocks.
if|if
condition|(
name|blocksInCurrentNode
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
comment|// Node is done. All blocks were fit into node-local splits.
name|completedNodes
operator|.
name|add
argument_list|(
name|node
argument_list|)
expr_stmt|;
block|}
comment|// else Run through the node again.
block|}
block|}
comment|// Check if node-local assignments are complete.
if|if
condition|(
name|completedNodes
operator|.
name|size
argument_list|()
operator|==
name|totalNodes
operator|||
name|totalLength
operator|==
literal|0
condition|)
block|{
comment|// All nodes have been walked over and marked as completed or all blocks
comment|// have been assigned. The rest should be handled via rackLock assignment.
name|LOG
operator|.
name|info
argument_list|(
literal|"DEBUG: Terminated node allocation with : CompletedNodes: "
operator|+
name|completedNodes
operator|.
name|size
argument_list|()
operator|+
literal|", size left: "
operator|+
name|totalLength
argument_list|)
expr_stmt|;
break|break;
block|}
block|}
comment|// if blocks in a rack are below the specified minimum size, then keep them
comment|// in 'overflow'. After the processing of all racks is complete, these
comment|// overflow blocks will be combined into splits.
name|ArrayList
argument_list|<
name|OneBlockInfo
argument_list|>
name|overflowBlocks
init|=
operator|new
name|ArrayList
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|racks
init|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|// Process all racks over and over again until there is no more work to do.
while|while
condition|(
name|blockToNodes
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|// Create one split for this rack before moving over to the next rack.
comment|// Come back to this rack after creating a single split for each of the
comment|// remaining racks.
comment|// Process one rack location at a time, Combine all possible blocks that
comment|// reside on this rack as one split. (constrained by minimum and maximum
comment|// split size).
comment|// iterate over all racks
for|for
control|(
name|Iterator
argument_list|<
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
argument_list|>
name|iter
init|=
name|rackToBlocks
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|iter
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|one
init|=
name|iter
operator|.
name|next
argument_list|()
decl_stmt|;
name|racks
operator|.
name|add
argument_list|(
name|one
operator|.
name|getKey
argument_list|()
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|OneBlockInfo
argument_list|>
name|blocks
init|=
name|one
operator|.
name|getValue
argument_list|()
decl_stmt|;
comment|// for each block, copy it into validBlocks. Delete it from
comment|// blockToNodes so that the same block does not appear in
comment|// two different splits.
name|boolean
name|createdSplit
init|=
literal|false
decl_stmt|;
for|for
control|(
name|OneBlockInfo
name|oneblock
range|:
name|blocks
control|)
block|{
if|if
condition|(
name|blockToNodes
operator|.
name|containsKey
argument_list|(
name|oneblock
argument_list|)
condition|)
block|{
name|validBlocks
operator|.
name|add
argument_list|(
name|oneblock
argument_list|)
expr_stmt|;
name|blockToNodes
operator|.
name|remove
argument_list|(
name|oneblock
argument_list|)
expr_stmt|;
name|curSplitSize
operator|+=
name|oneblock
operator|.
name|length
expr_stmt|;
comment|// if the accumulated split size exceeds the maximum, then
comment|// create this split.
if|if
condition|(
name|maxSize
operator|!=
literal|0
operator|&&
name|curSplitSize
operator|>=
name|maxSize
condition|)
block|{
comment|// create an input split and add it to the splits array
name|addCreatedSplit
argument_list|(
name|splits
argument_list|,
name|getHosts
argument_list|(
name|racks
argument_list|)
argument_list|,
name|validBlocks
argument_list|)
expr_stmt|;
name|createdSplit
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
block|}
comment|// if we created a split, then just go to the next rack
if|if
condition|(
name|createdSplit
condition|)
block|{
name|curSplitSize
operator|=
literal|0
expr_stmt|;
name|validBlocks
operator|.
name|clear
argument_list|()
expr_stmt|;
name|racks
operator|.
name|clear
argument_list|()
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
operator|!
name|validBlocks
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
if|if
condition|(
name|minSizeRack
operator|!=
literal|0
operator|&&
name|curSplitSize
operator|>=
name|minSizeRack
condition|)
block|{
comment|// if there is a minimum size specified, then create a single split
comment|// otherwise, store these blocks into overflow data structure
name|addCreatedSplit
argument_list|(
name|splits
argument_list|,
name|getHosts
argument_list|(
name|racks
argument_list|)
argument_list|,
name|validBlocks
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// There were a few blocks in this rack that
comment|// remained to be processed. Keep them in 'overflow' block list.
comment|// These will be combined later.
name|overflowBlocks
operator|.
name|addAll
argument_list|(
name|validBlocks
argument_list|)
expr_stmt|;
block|}
block|}
name|curSplitSize
operator|=
literal|0
expr_stmt|;
name|validBlocks
operator|.
name|clear
argument_list|()
expr_stmt|;
name|racks
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
assert|assert
name|blockToNodes
operator|.
name|isEmpty
argument_list|()
assert|;
assert|assert
name|curSplitSize
operator|==
literal|0
assert|;
assert|assert
name|validBlocks
operator|.
name|isEmpty
argument_list|()
assert|;
assert|assert
name|racks
operator|.
name|isEmpty
argument_list|()
assert|;
comment|// Process all overflow blocks
for|for
control|(
name|OneBlockInfo
name|oneblock
range|:
name|overflowBlocks
control|)
block|{
name|validBlocks
operator|.
name|add
argument_list|(
name|oneblock
argument_list|)
expr_stmt|;
name|curSplitSize
operator|+=
name|oneblock
operator|.
name|length
expr_stmt|;
comment|// This might cause an exiting rack location to be re-added,
comment|// but it should be ok.
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|oneblock
operator|.
name|racks
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|racks
operator|.
name|add
argument_list|(
name|oneblock
operator|.
name|racks
index|[
name|i
index|]
argument_list|)
expr_stmt|;
block|}
comment|// if the accumulated split size exceeds the maximum, then
comment|// create this split.
if|if
condition|(
name|maxSize
operator|!=
literal|0
operator|&&
name|curSplitSize
operator|>=
name|maxSize
condition|)
block|{
comment|// create an input split and add it to the splits array
name|addCreatedSplit
argument_list|(
name|splits
argument_list|,
name|getHosts
argument_list|(
name|racks
argument_list|)
argument_list|,
name|validBlocks
argument_list|)
expr_stmt|;
name|curSplitSize
operator|=
literal|0
expr_stmt|;
name|validBlocks
operator|.
name|clear
argument_list|()
expr_stmt|;
name|racks
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
comment|// Process any remaining blocks, if any.
if|if
condition|(
operator|!
name|validBlocks
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|addCreatedSplit
argument_list|(
name|splits
argument_list|,
name|getHosts
argument_list|(
name|racks
argument_list|)
argument_list|,
name|validBlocks
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Create a single split from the list of blocks specified in validBlocks    * Add this new split into splitList.    */
DECL|method|addCreatedSplit (List<InputSplit> splitList, Collection<String> locations, ArrayList<OneBlockInfo> validBlocks)
specifier|private
name|void
name|addCreatedSplit
parameter_list|(
name|List
argument_list|<
name|InputSplit
argument_list|>
name|splitList
parameter_list|,
name|Collection
argument_list|<
name|String
argument_list|>
name|locations
parameter_list|,
name|ArrayList
argument_list|<
name|OneBlockInfo
argument_list|>
name|validBlocks
parameter_list|)
block|{
comment|// create an input split
name|Path
index|[]
name|fl
init|=
operator|new
name|Path
index|[
name|validBlocks
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|long
index|[]
name|offset
init|=
operator|new
name|long
index|[
name|validBlocks
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|long
index|[]
name|length
init|=
operator|new
name|long
index|[
name|validBlocks
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|validBlocks
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|fl
index|[
name|i
index|]
operator|=
name|validBlocks
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|onepath
expr_stmt|;
name|offset
index|[
name|i
index|]
operator|=
name|validBlocks
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|offset
expr_stmt|;
name|length
index|[
name|i
index|]
operator|=
name|validBlocks
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|length
expr_stmt|;
block|}
comment|// add this split to the list that is returned
name|CombineFileSplit
name|thissplit
init|=
operator|new
name|CombineFileSplit
argument_list|(
name|fl
argument_list|,
name|offset
argument_list|,
name|length
argument_list|,
name|locations
operator|.
name|toArray
argument_list|(
operator|new
name|String
index|[
literal|0
index|]
argument_list|)
argument_list|)
decl_stmt|;
name|splitList
operator|.
name|add
argument_list|(
name|thissplit
argument_list|)
expr_stmt|;
block|}
comment|/**    * This is not implemented yet.     */
DECL|method|createRecordReader (InputSplit split, TaskAttemptContext context)
specifier|public
specifier|abstract
name|RecordReader
argument_list|<
name|K
argument_list|,
name|V
argument_list|>
name|createRecordReader
parameter_list|(
name|InputSplit
name|split
parameter_list|,
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**    * information about one file from the File System    */
annotation|@
name|VisibleForTesting
DECL|class|OneFileInfo
specifier|static
class|class
name|OneFileInfo
block|{
DECL|field|fileSize
specifier|private
name|long
name|fileSize
decl_stmt|;
comment|// size of the file
DECL|field|blocks
specifier|private
name|OneBlockInfo
index|[]
name|blocks
decl_stmt|;
comment|// all blocks in this file
DECL|method|OneFileInfo (FileStatus stat, Configuration conf, boolean isSplitable, HashMap<String, List<OneBlockInfo>> rackToBlocks, HashMap<OneBlockInfo, String[]> blockToNodes, HashMap<String, Set<OneBlockInfo>> nodeToBlocks, HashMap<String, Set<String>> rackToNodes, long maxSize)
name|OneFileInfo
parameter_list|(
name|FileStatus
name|stat
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isSplitable
parameter_list|,
name|HashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|rackToBlocks
parameter_list|,
name|HashMap
argument_list|<
name|OneBlockInfo
argument_list|,
name|String
index|[]
argument_list|>
name|blockToNodes
parameter_list|,
name|HashMap
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|nodeToBlocks
parameter_list|,
name|HashMap
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|String
argument_list|>
argument_list|>
name|rackToNodes
parameter_list|,
name|long
name|maxSize
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|fileSize
operator|=
literal|0
expr_stmt|;
comment|// get block locations from file system
name|BlockLocation
index|[]
name|locations
decl_stmt|;
if|if
condition|(
name|stat
operator|instanceof
name|LocatedFileStatus
condition|)
block|{
name|locations
operator|=
operator|(
operator|(
name|LocatedFileStatus
operator|)
name|stat
operator|)
operator|.
name|getBlockLocations
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|FileSystem
name|fs
init|=
name|stat
operator|.
name|getPath
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|locations
operator|=
name|fs
operator|.
name|getFileBlockLocations
argument_list|(
name|stat
argument_list|,
literal|0
argument_list|,
name|stat
operator|.
name|getLen
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// create a list of all block and their locations
if|if
condition|(
name|locations
operator|==
literal|null
condition|)
block|{
name|blocks
operator|=
operator|new
name|OneBlockInfo
index|[
literal|0
index|]
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|locations
operator|.
name|length
operator|==
literal|0
operator|&&
operator|!
name|stat
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
name|locations
operator|=
operator|new
name|BlockLocation
index|[]
block|{
operator|new
name|BlockLocation
argument_list|()
block|}
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|isSplitable
condition|)
block|{
comment|// if the file is not splitable, just create the one block with
comment|// full file length
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"File is not splittable so no parallelization "
operator|+
literal|"is possible: "
operator|+
name|stat
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|blocks
operator|=
operator|new
name|OneBlockInfo
index|[
literal|1
index|]
expr_stmt|;
name|fileSize
operator|=
name|stat
operator|.
name|getLen
argument_list|()
expr_stmt|;
name|blocks
index|[
literal|0
index|]
operator|=
operator|new
name|OneBlockInfo
argument_list|(
name|stat
operator|.
name|getPath
argument_list|()
argument_list|,
literal|0
argument_list|,
name|fileSize
argument_list|,
name|locations
index|[
literal|0
index|]
operator|.
name|getHosts
argument_list|()
argument_list|,
name|locations
index|[
literal|0
index|]
operator|.
name|getTopologyPaths
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|ArrayList
argument_list|<
name|OneBlockInfo
argument_list|>
name|blocksList
init|=
operator|new
name|ArrayList
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|(
name|locations
operator|.
name|length
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|locations
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|fileSize
operator|+=
name|locations
index|[
name|i
index|]
operator|.
name|getLength
argument_list|()
expr_stmt|;
comment|// each split can be a maximum of maxSize
name|long
name|left
init|=
name|locations
index|[
name|i
index|]
operator|.
name|getLength
argument_list|()
decl_stmt|;
name|long
name|myOffset
init|=
name|locations
index|[
name|i
index|]
operator|.
name|getOffset
argument_list|()
decl_stmt|;
name|long
name|myLength
init|=
literal|0
decl_stmt|;
do|do
block|{
if|if
condition|(
name|maxSize
operator|==
literal|0
condition|)
block|{
name|myLength
operator|=
name|left
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|left
operator|>
name|maxSize
operator|&&
name|left
operator|<
literal|2
operator|*
name|maxSize
condition|)
block|{
comment|// if remainder is between max and 2*max - then
comment|// instead of creating splits of size max, left-max we
comment|// create splits of size left/2 and left/2. This is
comment|// a heuristic to avoid creating really really small
comment|// splits.
name|myLength
operator|=
name|left
operator|/
literal|2
expr_stmt|;
block|}
else|else
block|{
name|myLength
operator|=
name|Math
operator|.
name|min
argument_list|(
name|maxSize
argument_list|,
name|left
argument_list|)
expr_stmt|;
block|}
block|}
name|OneBlockInfo
name|oneblock
init|=
operator|new
name|OneBlockInfo
argument_list|(
name|stat
operator|.
name|getPath
argument_list|()
argument_list|,
name|myOffset
argument_list|,
name|myLength
argument_list|,
name|locations
index|[
name|i
index|]
operator|.
name|getHosts
argument_list|()
argument_list|,
name|locations
index|[
name|i
index|]
operator|.
name|getTopologyPaths
argument_list|()
argument_list|)
decl_stmt|;
name|left
operator|-=
name|myLength
expr_stmt|;
name|myOffset
operator|+=
name|myLength
expr_stmt|;
name|blocksList
operator|.
name|add
argument_list|(
name|oneblock
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|left
operator|>
literal|0
condition|)
do|;
block|}
name|blocks
operator|=
name|blocksList
operator|.
name|toArray
argument_list|(
operator|new
name|OneBlockInfo
index|[
name|blocksList
operator|.
name|size
argument_list|()
index|]
argument_list|)
expr_stmt|;
block|}
name|populateBlockInfo
argument_list|(
name|blocks
argument_list|,
name|rackToBlocks
argument_list|,
name|blockToNodes
argument_list|,
name|nodeToBlocks
argument_list|,
name|rackToNodes
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|VisibleForTesting
DECL|method|populateBlockInfo (OneBlockInfo[] blocks, Map<String, List<OneBlockInfo>> rackToBlocks, Map<OneBlockInfo, String[]> blockToNodes, Map<String, Set<OneBlockInfo>> nodeToBlocks, Map<String, Set<String>> rackToNodes)
specifier|static
name|void
name|populateBlockInfo
parameter_list|(
name|OneBlockInfo
index|[]
name|blocks
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|rackToBlocks
parameter_list|,
name|Map
argument_list|<
name|OneBlockInfo
argument_list|,
name|String
index|[]
argument_list|>
name|blockToNodes
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|>
name|nodeToBlocks
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|String
argument_list|>
argument_list|>
name|rackToNodes
parameter_list|)
block|{
for|for
control|(
name|OneBlockInfo
name|oneblock
range|:
name|blocks
control|)
block|{
comment|// add this block to the block --> node locations map
name|blockToNodes
operator|.
name|put
argument_list|(
name|oneblock
argument_list|,
name|oneblock
operator|.
name|hosts
argument_list|)
expr_stmt|;
comment|// For blocks that do not have host/rack information,
comment|// assign to default  rack.
name|String
index|[]
name|racks
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|oneblock
operator|.
name|hosts
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|racks
operator|=
operator|new
name|String
index|[]
block|{
name|NetworkTopology
operator|.
name|DEFAULT_RACK
block|}
expr_stmt|;
block|}
else|else
block|{
name|racks
operator|=
name|oneblock
operator|.
name|racks
expr_stmt|;
block|}
comment|// add this block to the rack --> block map
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|racks
operator|.
name|length
condition|;
name|j
operator|++
control|)
block|{
name|String
name|rack
init|=
name|racks
index|[
name|j
index|]
decl_stmt|;
name|List
argument_list|<
name|OneBlockInfo
argument_list|>
name|blklist
init|=
name|rackToBlocks
operator|.
name|get
argument_list|(
name|rack
argument_list|)
decl_stmt|;
if|if
condition|(
name|blklist
operator|==
literal|null
condition|)
block|{
name|blklist
operator|=
operator|new
name|ArrayList
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|()
expr_stmt|;
name|rackToBlocks
operator|.
name|put
argument_list|(
name|rack
argument_list|,
name|blklist
argument_list|)
expr_stmt|;
block|}
name|blklist
operator|.
name|add
argument_list|(
name|oneblock
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|racks
index|[
name|j
index|]
operator|.
name|equals
argument_list|(
name|NetworkTopology
operator|.
name|DEFAULT_RACK
argument_list|)
condition|)
block|{
comment|// Add this host to rackToNodes map
name|addHostToRack
argument_list|(
name|rackToNodes
argument_list|,
name|racks
index|[
name|j
index|]
argument_list|,
name|oneblock
operator|.
name|hosts
index|[
name|j
index|]
argument_list|)
expr_stmt|;
block|}
block|}
comment|// add this block to the node --> block map
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|oneblock
operator|.
name|hosts
operator|.
name|length
condition|;
name|j
operator|++
control|)
block|{
name|String
name|node
init|=
name|oneblock
operator|.
name|hosts
index|[
name|j
index|]
decl_stmt|;
name|Set
argument_list|<
name|OneBlockInfo
argument_list|>
name|blklist
init|=
name|nodeToBlocks
operator|.
name|get
argument_list|(
name|node
argument_list|)
decl_stmt|;
if|if
condition|(
name|blklist
operator|==
literal|null
condition|)
block|{
name|blklist
operator|=
operator|new
name|LinkedHashSet
argument_list|<
name|OneBlockInfo
argument_list|>
argument_list|()
expr_stmt|;
name|nodeToBlocks
operator|.
name|put
argument_list|(
name|node
argument_list|,
name|blklist
argument_list|)
expr_stmt|;
block|}
name|blklist
operator|.
name|add
argument_list|(
name|oneblock
argument_list|)
expr_stmt|;
block|}
block|}
block|}
DECL|method|getLength ()
name|long
name|getLength
parameter_list|()
block|{
return|return
name|fileSize
return|;
block|}
DECL|method|getBlocks ()
name|OneBlockInfo
index|[]
name|getBlocks
parameter_list|()
block|{
return|return
name|blocks
return|;
block|}
block|}
comment|/**    * information about one block from the File System    */
annotation|@
name|VisibleForTesting
DECL|class|OneBlockInfo
specifier|static
class|class
name|OneBlockInfo
block|{
DECL|field|onepath
name|Path
name|onepath
decl_stmt|;
comment|// name of this file
DECL|field|offset
name|long
name|offset
decl_stmt|;
comment|// offset in file
DECL|field|length
name|long
name|length
decl_stmt|;
comment|// length of this block
DECL|field|hosts
name|String
index|[]
name|hosts
decl_stmt|;
comment|// nodes on which this block resides
DECL|field|racks
name|String
index|[]
name|racks
decl_stmt|;
comment|// network topology of hosts
DECL|method|OneBlockInfo (Path path, long offset, long len, String[] hosts, String[] topologyPaths)
name|OneBlockInfo
parameter_list|(
name|Path
name|path
parameter_list|,
name|long
name|offset
parameter_list|,
name|long
name|len
parameter_list|,
name|String
index|[]
name|hosts
parameter_list|,
name|String
index|[]
name|topologyPaths
parameter_list|)
block|{
name|this
operator|.
name|onepath
operator|=
name|path
expr_stmt|;
name|this
operator|.
name|offset
operator|=
name|offset
expr_stmt|;
name|this
operator|.
name|hosts
operator|=
name|hosts
expr_stmt|;
name|this
operator|.
name|length
operator|=
name|len
expr_stmt|;
assert|assert
operator|(
name|hosts
operator|.
name|length
operator|==
name|topologyPaths
operator|.
name|length
operator|||
name|topologyPaths
operator|.
name|length
operator|==
literal|0
operator|)
assert|;
comment|// if the file system does not have any rack information, then
comment|// use dummy rack location.
if|if
condition|(
name|topologyPaths
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|topologyPaths
operator|=
operator|new
name|String
index|[
name|hosts
operator|.
name|length
index|]
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|topologyPaths
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|topologyPaths
index|[
name|i
index|]
operator|=
operator|(
operator|new
name|NodeBase
argument_list|(
name|hosts
index|[
name|i
index|]
argument_list|,
name|NetworkTopology
operator|.
name|DEFAULT_RACK
argument_list|)
operator|)
operator|.
name|toString
argument_list|()
expr_stmt|;
block|}
block|}
comment|// The topology paths have the host name included as the last
comment|// component. Strip it.
name|this
operator|.
name|racks
operator|=
operator|new
name|String
index|[
name|topologyPaths
operator|.
name|length
index|]
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|topologyPaths
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|this
operator|.
name|racks
index|[
name|i
index|]
operator|=
operator|(
operator|new
name|NodeBase
argument_list|(
name|topologyPaths
index|[
name|i
index|]
argument_list|)
operator|)
operator|.
name|getNetworkLocation
argument_list|()
expr_stmt|;
block|}
block|}
block|}
DECL|method|getFileBlockLocations ( FileSystem fs, FileStatus stat)
specifier|protected
name|BlockLocation
index|[]
name|getFileBlockLocations
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
name|stat
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|stat
operator|instanceof
name|LocatedFileStatus
condition|)
block|{
return|return
operator|(
operator|(
name|LocatedFileStatus
operator|)
name|stat
operator|)
operator|.
name|getBlockLocations
argument_list|()
return|;
block|}
return|return
name|fs
operator|.
name|getFileBlockLocations
argument_list|(
name|stat
argument_list|,
literal|0
argument_list|,
name|stat
operator|.
name|getLen
argument_list|()
argument_list|)
return|;
block|}
DECL|method|addHostToRack (Map<String, Set<String>> rackToNodes, String rack, String host)
specifier|private
specifier|static
name|void
name|addHostToRack
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|Set
argument_list|<
name|String
argument_list|>
argument_list|>
name|rackToNodes
parameter_list|,
name|String
name|rack
parameter_list|,
name|String
name|host
parameter_list|)
block|{
name|Set
argument_list|<
name|String
argument_list|>
name|hosts
init|=
name|rackToNodes
operator|.
name|get
argument_list|(
name|rack
argument_list|)
decl_stmt|;
if|if
condition|(
name|hosts
operator|==
literal|null
condition|)
block|{
name|hosts
operator|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|()
expr_stmt|;
name|rackToNodes
operator|.
name|put
argument_list|(
name|rack
argument_list|,
name|hosts
argument_list|)
expr_stmt|;
block|}
name|hosts
operator|.
name|add
argument_list|(
name|host
argument_list|)
expr_stmt|;
block|}
DECL|method|getHosts (Set<String> racks)
specifier|private
name|Set
argument_list|<
name|String
argument_list|>
name|getHosts
parameter_list|(
name|Set
argument_list|<
name|String
argument_list|>
name|racks
parameter_list|)
block|{
name|Set
argument_list|<
name|String
argument_list|>
name|hosts
init|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|rack
range|:
name|racks
control|)
block|{
if|if
condition|(
name|rackToNodes
operator|.
name|containsKey
argument_list|(
name|rack
argument_list|)
condition|)
block|{
name|hosts
operator|.
name|addAll
argument_list|(
name|rackToNodes
operator|.
name|get
argument_list|(
name|rack
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|hosts
return|;
block|}
comment|/**    * Accept a path only if any one of filters given in the    * constructor do.     */
DECL|class|MultiPathFilter
specifier|private
specifier|static
class|class
name|MultiPathFilter
implements|implements
name|PathFilter
block|{
DECL|field|filters
specifier|private
name|List
argument_list|<
name|PathFilter
argument_list|>
name|filters
decl_stmt|;
DECL|method|MultiPathFilter ()
specifier|public
name|MultiPathFilter
parameter_list|()
block|{
name|this
operator|.
name|filters
operator|=
operator|new
name|ArrayList
argument_list|<
name|PathFilter
argument_list|>
argument_list|()
expr_stmt|;
block|}
DECL|method|MultiPathFilter (List<PathFilter> filters)
specifier|public
name|MultiPathFilter
parameter_list|(
name|List
argument_list|<
name|PathFilter
argument_list|>
name|filters
parameter_list|)
block|{
name|this
operator|.
name|filters
operator|=
name|filters
expr_stmt|;
block|}
DECL|method|add (PathFilter one)
specifier|public
name|void
name|add
parameter_list|(
name|PathFilter
name|one
parameter_list|)
block|{
name|filters
operator|.
name|add
argument_list|(
name|one
argument_list|)
expr_stmt|;
block|}
DECL|method|accept (Path path)
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
for|for
control|(
name|PathFilter
name|filter
range|:
name|filters
control|)
block|{
if|if
condition|(
name|filter
operator|.
name|accept
argument_list|(
name|path
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
DECL|method|toString ()
specifier|public
name|String
name|toString
parameter_list|()
block|{
name|StringBuffer
name|buf
init|=
operator|new
name|StringBuffer
argument_list|()
decl_stmt|;
name|buf
operator|.
name|append
argument_list|(
literal|"["
argument_list|)
expr_stmt|;
for|for
control|(
name|PathFilter
name|f
range|:
name|filters
control|)
block|{
name|buf
operator|.
name|append
argument_list|(
name|f
argument_list|)
expr_stmt|;
name|buf
operator|.
name|append
argument_list|(
literal|","
argument_list|)
expr_stmt|;
block|}
name|buf
operator|.
name|append
argument_list|(
literal|"]"
argument_list|)
expr_stmt|;
return|return
name|buf
operator|.
name|toString
argument_list|()
return|;
block|}
block|}
block|}
end_class

end_unit

