begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
DECL|package|org.apache.hadoop.contrib.index.mapred
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|contrib
operator|.
name|index
operator|.
name|mapred
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|contrib
operator|.
name|index
operator|.
name|lucene
operator|.
name|ShardWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Closeable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|WritableComparable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|MapReduceBase
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|OutputCollector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reducer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_comment
comment|/**  * This reducer applies to a shard the changes for it. A "new version" of  * a shard is created at the end of a reduce. It is important to note that  * the new version of the shard is not derived from scratch. By leveraging  * Lucene's update algorithm, the new version of each Lucene instance will  * share as many files as possible as the previous version.   */
end_comment

begin_class
DECL|class|IndexUpdateReducer
specifier|public
class|class
name|IndexUpdateReducer
extends|extends
name|MapReduceBase
implements|implements
name|Reducer
argument_list|<
name|Shard
argument_list|,
name|IntermediateForm
argument_list|,
name|Shard
argument_list|,
name|Text
argument_list|>
block|{
DECL|field|LOG
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|IndexUpdateReducer
operator|.
name|class
argument_list|)
decl_stmt|;
DECL|field|DONE
specifier|static
specifier|final
name|Text
name|DONE
init|=
operator|new
name|Text
argument_list|(
literal|"done"
argument_list|)
decl_stmt|;
comment|/**    * Get the reduce output key class.    * @return the reduce output key class    */
DECL|method|getOutputKeyClass ()
specifier|public
specifier|static
name|Class
argument_list|<
name|?
extends|extends
name|WritableComparable
argument_list|>
name|getOutputKeyClass
parameter_list|()
block|{
return|return
name|Shard
operator|.
name|class
return|;
block|}
comment|/**    * Get the reduce output value class.    * @return the reduce output value class    */
DECL|method|getOutputValueClass ()
specifier|public
specifier|static
name|Class
argument_list|<
name|?
extends|extends
name|Writable
argument_list|>
name|getOutputValueClass
parameter_list|()
block|{
return|return
name|Text
operator|.
name|class
return|;
block|}
DECL|field|iconf
specifier|private
name|IndexUpdateConfiguration
name|iconf
decl_stmt|;
DECL|field|mapredTempDir
specifier|private
name|String
name|mapredTempDir
decl_stmt|;
comment|/* (non-Javadoc)    * @see org.apache.hadoop.mapred.Reducer#reduce(java.lang.Object, java.util.Iterator, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)    */
DECL|method|reduce (Shard key, Iterator<IntermediateForm> values, OutputCollector<Shard, Text> output, Reporter reporter)
specifier|public
name|void
name|reduce
parameter_list|(
name|Shard
name|key
parameter_list|,
name|Iterator
argument_list|<
name|IntermediateForm
argument_list|>
name|values
parameter_list|,
name|OutputCollector
argument_list|<
name|Shard
argument_list|,
name|Text
argument_list|>
name|output
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Construct a shard writer for "
operator|+
name|key
argument_list|)
expr_stmt|;
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|iconf
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|temp
init|=
name|mapredTempDir
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
literal|"shard_"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
specifier|final
name|ShardWriter
name|writer
init|=
operator|new
name|ShardWriter
argument_list|(
name|fs
argument_list|,
name|key
argument_list|,
name|temp
argument_list|,
name|iconf
argument_list|)
decl_stmt|;
comment|// update the shard
while|while
condition|(
name|values
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|IntermediateForm
name|form
init|=
name|values
operator|.
name|next
argument_list|()
decl_stmt|;
name|writer
operator|.
name|process
argument_list|(
name|form
argument_list|)
expr_stmt|;
name|reporter
operator|.
name|progress
argument_list|()
expr_stmt|;
block|}
comment|// close the shard
specifier|final
name|Reporter
name|fReporter
init|=
name|reporter
decl_stmt|;
operator|new
name|Closeable
argument_list|()
block|{
specifier|volatile
name|boolean
name|closed
init|=
literal|false
decl_stmt|;
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
comment|// spawn a thread to give progress heartbeats
name|Thread
name|prog
init|=
operator|new
name|Thread
argument_list|()
block|{
specifier|public
name|void
name|run
parameter_list|()
block|{
while|while
condition|(
operator|!
name|closed
condition|)
block|{
try|try
block|{
name|fReporter
operator|.
name|setStatus
argument_list|(
literal|"closing"
argument_list|)
expr_stmt|;
name|Thread
operator|.
name|sleep
argument_list|(
literal|1000
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
continue|continue;
block|}
catch|catch
parameter_list|(
name|Throwable
name|e
parameter_list|)
block|{
return|return;
block|}
block|}
block|}
block|}
decl_stmt|;
try|try
block|{
name|prog
operator|.
name|start
argument_list|()
expr_stmt|;
if|if
condition|(
name|writer
operator|!=
literal|null
condition|)
block|{
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|closed
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
operator|.
name|close
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Closed the shard writer for "
operator|+
name|key
operator|+
literal|", writer = "
operator|+
name|writer
argument_list|)
expr_stmt|;
name|output
operator|.
name|collect
argument_list|(
name|key
argument_list|,
name|DONE
argument_list|)
expr_stmt|;
block|}
comment|/* (non-Javadoc)    * @see org.apache.hadoop.mapred.MapReduceBase#configure(org.apache.hadoop.mapred.JobConf)    */
DECL|method|configure (JobConf job)
specifier|public
name|void
name|configure
parameter_list|(
name|JobConf
name|job
parameter_list|)
block|{
name|iconf
operator|=
operator|new
name|IndexUpdateConfiguration
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|mapredTempDir
operator|=
name|iconf
operator|.
name|getMapredTempDir
argument_list|()
expr_stmt|;
name|mapredTempDir
operator|=
name|Shard
operator|.
name|normalizePath
argument_list|(
name|mapredTempDir
argument_list|)
expr_stmt|;
block|}
comment|/* (non-Javadoc)    * @see org.apache.hadoop.mapred.MapReduceBase#close()    */
DECL|method|close ()
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{   }
block|}
end_class

end_unit

